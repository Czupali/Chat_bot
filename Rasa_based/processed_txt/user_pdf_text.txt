SeTAR: Out-of-Distribution Detection with
Selective Low-Rank Approximation
YixiaLi1‚àó, BoyaXiong2‚àó, GuanhuaChen1‚Ä†, YunChen3‚Ä†
1SouthernUniversityofScienceandTechnology
2ShanghaiUniversityofFinanceandEconomics
3MoEKeyLaboratoryofInterdisciplinaryResearchofComputationandEconomics,
ShanghaiUniversityofFinanceandEconomics
liyixia@me.com, xiongboya@163.sufe.edu.cn
chengh3@sustech.edu.cn, yunchen@sufe.edu.cn
Abstract
Out-of-distribution(OOD)detectioniscrucialforthesafedeploymentofneural
networks. ExistingCLIP-basedapproachesperformOODdetectionbydevising
novelscoringfunctionsorsophisticatedfine-tuningmethods. Inthiswork,wepro-
poseSeTAR,anovel,training-freeOODdetectionmethodthatleveragesselective
low-rank approximation of weight matrices in vision-language and vision-only
models. SeTARenhancesOODdetectionviapost-hocmodificationofthemodel‚Äôs
weightmatricesusingasimplegreedysearchalgorithm. BasedonSeTAR,we
furtherproposeSeTAR+FT,afine-tuningextensionoptimizingmodelperformance
forOODdetectiontasks. ExtensiveevaluationsonImageNet1KandPascal-VOC
benchmarks show SeTAR‚Äôs superior performance, reducing the relatively false
positiveratebyupto18.95%and36.80%comparedtozero-shotandfine-tuning
baselines. AblationstudiesfurthervalidateSeTAR‚Äôseffectiveness,robustness,and
generalizability across different model backbones. Our work offers a scalable,
efficientsolutionforOODdetection,settinganewstate-of-the-artinthisarea.
1 Introduction
Thetaskofout-of-distribution(OOD)detection(Hendrycks&Gimpel,2017;Mingetal.,2022)
aimstoidentifywhetherinputdatacomesfromanunknowndistribution. Ithasgarneredsignificant
attentioninthemachinelearningcommunityrecently(Hendrycksetal.,2020;Xuetal.,2021;Miyai
etal.,2023a). Whilemachinelearningmodelsaretrainedwithsupervisedin-distribution(ID)data,
theyoftenstruggletogeneralizetoOODdataencounteredinreal-worldapplications(Emmottetal.,
2016)likeautonomousvehiclesandhealthcare. TheseOODsamplesposechallengesastheyare
notrepresentedinthetrainingdata. Consequently,OODdetectionplaysacrucialroleindeveloping
reliable and trustworthy machine-learning models suitable for real-world deployment (Bai et al.,
2023). Itallowsmodelstofilteroutandrejecttheseawkwardinputseffectively,andenablestheuse
ofcuratedandlabeledOODsamplestofurthertrainforamorerobustmodelinthewild.
PreviousresearchhasprimarilyfocusedondetectingOODinstancesineithervisual(DeVries&
Taylor,2018;Liangetal.,2018;Hendrycksetal.,2022)ortextualdata(Hu&Khan,2021;Zheng
etal.,2020;Zhouetal.,2021). Recently,significantprogresshasbeenmadeinmultimodaltaskslike
multimodalretrieval(Lietal.,2023;Caesaretal.,2018)andimageclassification(Yuetal.,2022),
thankstovision-and-languagepretrained(VLP)modelslikeCLIP(Radfordetal.,2021). Morerecent
‚àóEqualContribution.
‚Ä†CorrespondingAuthors.
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
studieshaveexploredOODdetectionwithCLIP,groupedintozero-shotmethods(Fortetal.,2021;
Mingetal.,2022;Miyaietal.,2023b)andfinetuning-basedmethods(Ming&Li,2023;Taoetal.,
2023;Miyaietal.,2023a). However,thezero-shotmethodssufferfromsuboptimalperformancedue
topotentialdomaingapswithIDdownstreamdata.Ontheotherhand,finetuning-basedmethodscarry
theriskofdeconstructingtheintricaterepresentationslearnedbyCLIPwhichrequiresameticulously
designedtrainingstrategy. Sparsification-basedapproaches(Sunetal.,2021;Djurisicetal.,2023)
have demonstrated potential in OOD detection within CNNs, leveraging the assumption that ID
andOODsamplesproducedistinctactivationpatterns. Nevertheless,theireffectivenessdiminishes
inlarge-scalepre-trainedmodelssuchasCLIP,whereactivationdifferencesbecomemoresubtle,
therebylimitingtheirapplicabilityprimarilytomodelsfine-tunedondownstreamID-domaindatasets.
Inthiswork,weproposeSeTAR,atraining-freeandeffectiveOODdetectionmethodbyselective
low-rankapproximations. Low-rankapproximationistoapproximateagivenmatrixbyfindinga
lower-rankmatrixthatcloselyresemblestheoriginalmatrix. Previousresearchhasdemonstratedthat
usinglow-rankapproximationmatricescanachievecomparableperformancetofullparametersin
variousscenarios,asobservedintaskssuchaslargelanguagemodel(LLM)fine-tuning(Huetal.,
2022)andmodelpruning(Hajimolahoseinietal.,2021). Theseapproachestypicallypreservethe
samerankacrossdifferentlow-rankapproximationmatrices. Inourwork,wedemonstratethatit
ispossibletosignificantlyenhancetheperformanceofOODdetectionbyselectivelymanipulating
the weight matrices in the CLIP model, including the choice of weight matrices and the ratio of
singularvectorstobereduced. Specifically,weproposeasimpletop-to-bottomandimage-to-text
greedysearchalgorithmtomanipulateW intheCLIPmodel. Ourmethodappliestovariousmodel
up
backbonesanddoesnotrequireanyadditionaltrainingornewparameters. BuildinguponSeTAR,
wefurtherdemonstrateitseffectivenessforfine-tuninginitialization,referredtoasSeTAR+FT.
Weconductextensiveevaluationsandachievestate-of-the-artperformanceoncommonOODdetec-
tionbenchmarksforCLIP,includingtheImageNet1KandPascal-VOCbenchmarks. Comparedto
vanillaMCMandGL-MCM,SeTARwiththeCLIPbackbonereducesrelativelyFPR95by9.5%
and12.0%onaverageacrosstwobenchmarks,respectively. Whenfurtherintegratefine-tuninginto
SeTAR, SeTAR+FT outperforms the state-of-the-art fine-tuning baselines LoCoOp (Miyai et al.,
2023a) and LoRA (Hu et al., 2022). Moreover, we perform a comprehensive ablation study and
analysistoverifyandunderstandSeTAR.Insummary,ourkeyresultsandcontributions:
1. WeproposeSeTAR,asimpleyeteffectiveOODdetectionmethodbasedonselectivelow-
rankapproximation. Itistraining-freeasitonlyperformspost-hocmodificationtoweight
matrices. SeTARappliestoavarietyofscoringfunctionsandmodelbackbones. Itcanbe
readilyintegratedwithexistingzero-shotOODdetectionmethods.
2. WefurtherextendSeTARtoSeTAR+FT,whichdemonstratestheeffectivenessofSeTARin
improvingtheperformanceoffinetuning-basedOODdetectionmethodsandachievingnew
state-of-the-artresults.
3. WeextensivelyevaluateSeTARandSeTAR+FTacrossadiversesetofOODdetectiontasks.
Itconsistentlyoutperformsbaselinemethodsandestablishesnewstate-of-the-artresultson
CLIP-basedOODdetectionbenchmarks. OnImageNet1K,SeTARachievesanAUROCof
91.32%withCLIPbackboneandGL-MCMscore. Thescorefurtherincreasesto92.31%
whencombinedwiththefinetuning-baseddetectionmethod.
4. We perform comprehensive ablation studies and empirical analyses to verify and under-
standSeTAR.Wehopethatthisworkwillshedlightonfutureexplorationsonin-depth
understandingoftheSeTARmethod.3
2 Preliminaries
CLIPArchitecture TheCLIPmodel(Radfordetal.,2021)comprisesanimageencoderEv(¬∑)
andatextencoderEt(¬∑),alignedviacontrastivelearningonweb-scaleimage-textpairs. Wefocus
onCLIP-ViT,wheretheimageencoderisaVisionTransformer(ViT).EachViTlayerincludesa
multiheadself-attentionsublayerandafeed-forwardsublayer.Intheself-attentionmodule,thehidden
stateisprojectedintodifferentspacesusinglearnableparametersW ,W ,W . Theoutputsare
q k v
3 Codeareavailableat https://github.com/X1AOX1A/SeTAR.
2
ùëÅ√ó +
FeedForward W(cid:2914)(cid:2925)(cid:2933)(cid:2924) W (cid:2931)(cid:2926) = U Œ£ V(cid:2904)
Down Projection Low-rank
Approximation
LayerNorm
W(cid:3553) = U‚Ä≤ Œ£‚Ä≤ V(cid:4593)(cid:2904)
+ Activation Function (cid:2931)(cid:2926)
Multi-head (c)
Attention
Up Projection
W(cid:2931)(cid:2926)
LayerNorm
Patch/Token Œ£ Œ£‚Ä≤
Embedding
(a) (b) (d)
Figure 1: The overview of SeTAR. (a) The structure of the CLIP image and text encoder. (b)
Thedetailsofthefeed-forwardsublayer. (c)Foreachencoderlayer, wereplacetheW weight
up
matrixwithitslow-rankapproximationW(cid:99)up. (d)TheillustrationofŒ£beforeandafterlow-rank
approximation. MoredetailsareinSection3.1.
concatenatedandprojectedbackwithanotherlinearmatrixW . Thefeed-forwardmoduleprojects
o
thehiddenstateintoawiderspaceusingW andthenbackwithW afteranon-linearactivation
up down
(Figure 1). Given the similarity between the image and text encoder layers, we adopt consistent
notationsforthelinearmatricesinboth. EachencoderalsoincludesalinearprojectorW tomap
p
theirrepresentationsintoasharedspaceforcontrastivelearning.
Zero-shotOODDetectionwithCLIP Zero-shotOODdetectionaimstoseparateIDandOOD
datawithoutanIDtrainingdataset. GiventheCLIP,theIDclassesaredefinedbytheclassification
taskofinterest,whichdiffersfromtheclassesusedinCLIPpretraining. Accordingly,OODisdefined
asclassesnotbelongingtoanyIDclass,makingtheOODdetectorabinaryclassifier. MCM(Ming
et al., 2022) and GL-MLM (Miyai et al., 2023b) are two zero-shot CLIP-based OOD detection
methods. Formally,letxbethetestimageandT ={y }K bethesetoftextpromptscontaining
in c c=1
M IDclasslabels(e.g., "aphotoofa[CLASS]"). Theimageissegmentedintol imagepatches
x=(x ,...,x ). FollowingCLIP,weadd[cls]beforetheimagepatchesandusetheoutputof[cls]
1 l
fromthevisualprojectorW astheglobalimagefeature(hv ‚ààRd). Theoutputsofotherpatchesare
p
projectedbythevisualprojectorasthelocalimagefeatures(pv =(pv,...,pv)‚ààRl√ód). Forthetext
1 l
prompty ‚ààT ,weaddanadditional[eos]afterthetexttokensandusetheoutputfeatureof[eos]
c in
fromthetextualprojectorW astheconceptfeatureofIDclassc(ht ‚ààRd).
p c
Thelabel-wiseimage-conceptmatching(IWIC)scoremeasureshowwellatestimagexalignswitha
concepty ,usingeitherglobalorlocalfeatures. TheglobalIWICscoresG(¬∑)isthecosinesimilarity
c c
betweentheglobalimagefeaturehv andtheconceptfeatureht: sG(x) = cos_sim(hv,ht). The
c c c
localIWICscoresL(¬∑)isthemax-pooledcosinesimilaritybetweenimagepatchfeaturespv andthe
c i
conceptfeatureht: sL(x)=max cos_sim(pv,ht). TheMCMandGL-MCMscoresaredefinedas:
c c i i c
esG c(x)/œÑ
S (x)=max , (1)
MCM c (cid:80)K c=1esG c(x)/œÑ
esL c(x)/œÑ‚Ä≤
S (x)=S (x)+max , (2)
GL‚àíMCM MCM c (cid:80)K c=1esL c(x)/œÑ‚Ä≤
whereœÑ andœÑ‚Ä≤arethetemperaturehyperparameters. MCMonlyusesglobalimagefeatures,while
GL-MCMadditionallyconsiderslocalimagefeatures. ForIDdata,bothMCMandGL-MCMscores
willbematchedtooneoftheconceptfeatureswithahighscore;andviceversa. Asaresult,our
OODdetectionfunctioncanbeformulatedas:
(cid:26)
1 S(x)‚â•Œª
G(x)= , (3)
0 S(x)<Œª
whereS(x)iseithertheMCMorGL-MCMscore,Œªisthethresholdvalue.Byconvention,G(x)=1
representstheIDclassandG(x)=0indicatestheOODclass. TheŒªischosensothatahighfraction
ofIDdata(e.g.,95%)isabovethethreshold. Wefollowpreviouswork(Miyaietal.,2023a)touse
eitherMCMorGL-MCMscoreforOODdetectioninthiswork.
3
3 Method
WeintroduceSeTAR,atraining-freeandeffectivetechniqueforimprovingOODdetectionperfor-
mance(seeFigure1). Ourkeyideaistoperformpost-hocmodificationtoCLIPweightmatrices
byselectivelyreplacingthemwiththeirlow-rankapproximations. Itiscomplementarytoexisting
CLIP-basedzero-shotOODdetectionmethodsandcouldbefurtherextendedtofinetuning-based
methods,whichwetermasSeTAR+FT.
3.1 OODDetectionwithSelectiveLow-RankApproximation
Low-RankApproximation GivenalinearmatrixW ‚ààRm√ón,itsSingularValueDecomposition
(SVD)isdenotedasW =UŒ£V‚ä§,whereU =[u ,u ,¬∑¬∑¬∑ ,u ]‚ààRm√óm,V =[v ,v ,¬∑¬∑¬∑ ,v ]‚àà
1 2 m 1 2 n
Rn√ón,andŒ£ ‚àà Rm√ón isamatrixwhoseentriesareallzeroexceptforthesingularvaluesofW.
Thesesingularvaluesappearindecreasingorderonthediagonal(i.e. œÉ‚Üì(W)). TheSVDofW can
i
bereformulatedasinEquation4. Givenahyperparameterr ‚àà N+,arank-rapproximationofW
ismatrixW(cid:99) thatminimizes‚à•W ‚àíW(cid:99)‚à•
2
andsatisfiesrank(W(cid:99)) ‚â§ r. Theoptimalsolutionofthis
problemW(cid:99)isprovidedbyEckart‚ÄìYoung‚ÄìMirskytheorem(Low-RankApproximation,2024)using
SingularValueDecomposition(seeEquation5).
min(m,n)
(cid:88)
W = œÉ‚Üì(W)u v‚ä§, (4)
i i i
i=1
r
(cid:88)
W(cid:99) = œÉ i‚Üì(W)u iv i‚ä§. (5)
i=1
In this work, we will use the term minor singular components to refer to entries in the SVD
correspondingtosmallsingularvalues. Thesecomponentsareremovedinlow-rankapproximation.
ThetermofprinciplesingularcomponentsisusedtorefertoentriesintheSVDcorrespondingto
largesingularvalues. Thesecomponentsarekeptinalow-rankapproximationofthematrix.
OODDetectionwithSelectiveLow-RankApproximation SVD-basedweightpruning,particu-
larlyinnoise-pronelayers,cansubstantiallyreduceanetwork‚Äôssensitivitytominorperturbations,
leadingtoenhancedstabilityandrobustness(Yaoetal.,2024). ThisstabilityiscrucialforOOD
detection,asitensuresthemodel‚Äôsreliableperformanceacrossawiderangeofinputs. Buildingon
this,weproposeamethodtoimproveOODdetectionbyselectivelyapplyinglow-rankapproximation
toweightmatrices. BydecomposingaweightmatrixW intoitssingularvaluesandvectors, we
canidentifyandretaintheprinciplesingularcomponentsthatsignificantlycontributetothemodel‚Äôs
performance. ThisapproachensuresthattheessentialfeaturesofW arepreservedwhilediscarding
thelesscriticalminorsingularcomponents. GivenaweightmatrixW inCLIP(e.g.,W orW ),we
up k
replacethematrixwithitslow-rankapproximationpartW(cid:99)asdescribedinEquation5(seeFigure1).
GiventherankreductionratioŒò,therankofW(cid:99)isdeterminedbyr(W(cid:99))=round((1‚àíŒò)¬∑r(W)).
Thisselectivelow-rankapproximationleveragesthecompactrepresentationprovidedbySVDto
enhancethemodel‚ÄôsabilitytodetectOODinstanceseffectivelywithoutrequiringadditionaltraining.
We demonstrate our method‚Äôs ability to improve OOD detection (Table 1) while maintaining ID
classificationperformance(Table7)inSection4.2andSection4.5.
HyperParameterSearchAlgorithm DuetothepresenceofmanyweightmatricesinCLIP,each
consistingofhundredsofsingularvalues,conductingacompletesearchoverallcombinationsof
low-rank approximation weight matrices is impractical. Therefore, we propose a greedy search
algorithmtodeterminetherankreductionratioforeachweightmatrix. Amongalllinearweight
matricesineachencoderlayer,wefocusonW asitismosteffectiveaccordingtoourpreliminary
up
experiment. Forsimplicity,weassumebothimageandtextencodershaveN encoderlayers. As
shown in Algorithm 1, we search by first enumerating all N vision encoder layers sequentially
from top to bottom and then all N text encoder layers in the same way. This search order is
concisely denoted as searching from 2N to the first layer in CLIP. We compare different search
algorithmsinSection4.4. TherankreductionratioforeachlayeristheobjectiveinSeTARwhichis
searchedamongthecandidatelistŒò={Œò ,Œò ,¬∑¬∑¬∑ ,Œò }accordingtothelossonthevalidation
0 1 J
set. We employ the LoCoOp loss (Equation 12) proposed in (Miyai et al., 2023a) as our loss
4
function. This loss requires only ID images. It contains an ID loss for ID image classification
and an OOD loss to push away pseudo OOD features from the ID class text embeddings where
the pseudo OOD features are from ID-irrelevant nuisances (Equation 10) (e.g., backgrounds) in
CLIP‚Äôslocalfeatures. WereferthereaderstoMiyaietal.(2023a)orAppendixBformoredetails.
For Œò ‚àà Œò, we remove Œò (in percent) sin-
j j
Algorithm1ThehyperparametersearchinSeTAR.
gularvaluesalongwiththeircorrespondingsin-
gular vectors to obtain the approximated ma- Data:ValidsetD.
Input:Layerlength2N,rankreductionratiocandidates
trix W(cid:99)up (Equation 5). It is worth noting that
ŒòwithlengthJ,lossL onDWITHOUTSe-
0
the rank reduction raio candidate list includes TAR.
Œò
0
= 0, indicatingthattheweightmatrixhas Result:RankreductionratiolistT‚àówithlength2N.
thechancetoremainunmodified. L‚àó=L ; // Current best loss
0
forLayerNuml‚Üê2N to1do
With the searched rank reduction ratio, the
weight matrix W in each CLIP layer is re-
W(cid:99)‚àó=W ul
p
T‚àó[l]=0
up forcounterj‚Üê1toJ do
placedandupdatedwithitsapproximation. The r=round((1‚àíŒò[j])¬∑rank(Wl ))
up
S bae cT kA bR onc ea sn (Tbe abe la es 8il )y
,
a bp ypl ri ee pd lat co ind giff te hr een mt oV di eT
l
W(cid:99)=(cid:80)r i=1œÉ i‚Üìu iv i‚ä§
CalcluatelossLl onDbyreplacingWl with
weightmatriceswiththeirlow-rankapproxima- j up
tions in a similar approach. Then SeTAR de-
W(cid:99)
ifLl <L‚àóthen
tects the OOD data samples following MCM j
(Equation1), GL-MCM(Equation2)orother
W(cid:99)‚àó=W(cid:99);T‚àó[l]=Œò[j];L‚àó=Ll j;
end
scoring-basedOODdetectionmethodwiththe
end
approximatedmodel. Weprovideanexample
procedureofthegreedysearchinListing1for
W ul p:=W(cid:99)‚àó
end
betterunderstanding.
returnT‚àó
3.2 OODDetectionwithSeTAR-enhancedLow-rankAdaptation
SeTAR canbefurthercombinedwithLoRA(Huetal.,2022)asanovellow-rankadaptationmethod
for OOD detection, which we refer to as SeTAR+FT. Specifically, we first apply SeTAR to the
pre-trainedCLIPmodeltoobtainthereservedrankrforeachweightmatrixW. Thenwehave
W =W(cid:99)+B√óA (6)
min(m,n)(cid:113)
(cid:88)
B = œÉ‚Üì(W)u (7)
i i
i=r+1
min(m,n)(cid:113)
(cid:88)
A= œÉ‚Üì(W)v‚ä§ (8)
i i
i=r+1
whereW(cid:99) isthelow-rankapproximationofW foundbySeTAR,withAandB beingtheminor
singularcomponents. Duringfinetuning,wekeepW(cid:99)frozenandonlyupdatethelow-rankmatrixA
andB. Inthisway,weretaintheprinciplesingularcomponentsintheoriginalweightmatrixand
onlyupdatetheminorsingularcomponents.UnlikeLoRA,whichevenlydistributesthefinetuning
rankbudgetacrossalllayers,SeTAR+FTadjuststherankforeachlayer,resultinginmoreeffective
andefficientfine-tuning(Table2andFigure6). MoredetailsareprovidedinSection4.3.
4 Experiments
4.1 ExperimentalSetup
Dataset Followingpreviouswork(Mingetal.,2022;Miyaietal.,2023b),weusetworeal-world
datasetscreatedfromImageNet1K(Dengetal.,2009)andPascal-VOC(Everinghametal.,2009)
astheIDdatasets. ForOODdatasets,wefollowMingetal.(2022)topreprocessiNaturalist,SUN,
PlacesandTexture,andfollowMiyaietal.(2023b)topreprocessImageNet22KandCOCOdata.
Forfinetuneexperiments,wefollowMiyaietal.(2023a)touseImageNet1KastheIDdataset. The
detaileddescriptionandstatisticsofthedatasetsareprovidedinAppendixC.
5
Table1: Training-freeresultsofFPR95(FPR)andAUROC(AUC)comparedtozero-shotbase-
linesonCLIP-base. Boldvaluesrepresentthehighestperformance. ‚Ä† iscitedfromMiyaietal.
(2023b),where‚ãÑrepresentstheabsenceofreportinginthepaper. ‚àódenotestheresultofourre-run.
‚àídenotestheOODdatasethasoverlappingcategorieswiththeIDdataset. Wedonotreportstandard
deviationssincenotrainingisinvolved.
iNaturalist SUN Places Texture ImageNet22K COCO Average
Method
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
ImageNet1K
MCMScore
VanillaMCM‚Ä† 30.91 94.61 37.59 92.57 44.69 89.77 57.77 86.11 - - - - 42.74 90.77
VanillaMCM‚àó 32.07 94.43 38.65 92.37 43.73 90.03 57.89 86.13 - - - - 43.09 90.74
SeTAR 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
GL-MCMScore
VanillaGL-MCM‚Ä† 15.18 96.71 30.42 93.09 38.85 89.90 57.93 83.63 - - - - 35.47 90.83
VanillaGL-MCM‚àó 15.34 96.62 30.65 93.01 37.76 90.07 57.41 83.73 - - - - 35.29 90.86
SeTAR 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
Pascal-VOC
MCMScore
VanillaMCM‚Ä† 8.20 98.23 28.60 94.68 ‚ãÑ ‚ãÑ 51.70 91.45 51.40 90.94 54.50 89.02 38.88 92.86
VanillaMCM‚àó 7.24 98.23 27.91 94.56 32.40 92.45 51.61 91.89 50.60 91.42 53.70 89.30 37.24 92.98
SeTAR 4.59 98.71 24.91 95.15 28.46 93.21 40.44 93.58 48.25 92.08 48.10 89.70 32.46 93.74
GL-MCMScore
VanillaGL-MCM‚Ä† 4.20 98.71 23.10 94.66 ‚ãÑ ‚ãÑ 43.00 92.84 41.00 92.38 44.30 90.48 31.12 93.81
VanillaGL-MCM‚àó 4.33 98.81 22.94 94.63 26.20 93.11 41.61 92.88 37.88 93.17 43.70 90.71 29.44 93.88
SeTAR 3.66 98.96 21.93 94.81 25.04 93.62 20.35 96.36 31.47 94.31 40.70 91.19 23.86 94.87
Settings Followingexistingstudies(Mingetal.,2022;Miyaietal.,2023b,a),weuseCLIPViT-
B/164(Radfordetal.,2021)asourbackbone. Bothimageandtextencodershave12layers. More
resultswithdifferentbackbonesareinSection4.4. Therankreductionratiocandidatesrangefrom
0to40%in5%intervals. Weuseatemperatureof15,unlessstatedotherwise. Inallexperiments,
weuseoneCLIPtextprompt: "aphotoofa[CLASS],",where[CLASS]istheIDclassname. We
sethyperparametersŒª(Equation12)andtop-K(Equation10)accordingtothespecificIDdatasets
and backbones. Detailed settings are in Table 12, with a sensitivity analysis in Section 4.4. For
SeTAR+FTandLoRAexperiments,thelearningrateandepochnumberaresetto1e‚àí2and5for
allexperiments. TheLoRArankrissettomatchthetrainableparametersofSeTAR+FT.Detailed
settingsareinTable13. Wereportresultsfromthreerunswithseeds3,4,56. Allexperimentsare
conductedonasingleNVIDIARTX4090GPU.Thetimecostforlow-rankapproximationwith
CLIP-baseontheImageNet1Kvalidationsetisabout20minutes.
Metrics Weusethefollowingmetricsforevaluation. (1)thefalsepositiverate(FPR95)forout-of-
distribution(OOD)samplesatafixedtruepositiverate(TPR)of95%forin-distributionsamples,with
lowervaluestargetingbetterperformance;and(2)theareaunderthereceiveroperatingcharacteristic
curve(AUROC)forOODsamples,withhighervaluesindicatingbetterperformance.
Baselines We evaluate SeTAR against MCM (Ming et al., 2022) and GL-MCM (Miyai et al.,
2023b),state-of-the-artzero-shotOODdetectionmethodsonCLIP.WealsocompareSeTAR+FT
withfine-tuningbaselinesNPOS(Taoetal.,2023),CoOp(Zhouetal.,2022),LoCoOp(Miyaietal.,
2023a),andLoRA(Huetal.,2022). MoredetailsareinAppendixD.
4.2 Training-freeResults
Thetraining-freeOODdetectionperformancesaresummarizedinTable1. Comparedwithzero-shot
baselines,asalientobservationisthatonbothMCMandGL-MCM,usingSeTARoutperformsthe
vanillamethodbyalargemarginacrossallOODdetectiontasks. Forexample,usingPascal-VOCas
ID,SeTARyieldsarelativelyaveragereductionof12.84%FPR95onMCMand18.95%FPR95on
GL-MCM.ConsideringthatSeTARisgenerallyapplicableandtraining-free,theseresultsarevery
encouraging. ComparingSeTARwithscoringfunctionMCMandGL-MCM,SeTAR+GL-MCM
performs better on all OOD detection tasks. However, the superiority of GL-MCM score over
MCMappearstobecontingentuponthechoiceofthemodelbackbone. AsevidencedinTable8,
SeTAR+MCM demonstrates superior performance with a relatively average FPR95 reduction of
8.30%comparedtoSeTAR+GL-MCMwithCLIP-largeasthebackboneonImageNet1K.
4https://huggingface.co/openai/clip-vit-base-patch16
5Temperatureissetto1.0forthescaledCLIPlogits,equivalenttotheunscaledCLIPlogitswithatemperature
of100.Weadopttheunscaledsettinginourimplementation.
6ForSeTAR,theresultsarethesameunderdifferentrandomseedsasitdoesnotrequiretraining.
6
4.3 Fine-tuningResults
In this section, we compare SeTAR+FT with fine-tuning baselines, including NPOS (Tao et al.,
2023), CoOp (Zhou et al., 2022), LoCoOp (Miyai et al., 2023a) and LoRA (Hu et al., 2022).
LoCoOp is the state-of-the-art prompt-learning OOD detection method on CLIP. LoRA is a
representative parameter-efficient fine-tuning method. Following previous work (Tao et al.,
2023; Zhou et al., 2022; Miyai et al., 2023a), we report the results on the the ImageNet1K
benchmark in Table 2. We observe that SeTAR+FT outperforms all baselines on both MCM
and GL-MCM scoring functions. For example, with CLIP-base as the backbone, SeTAR+FT
achieves a relatively average FPR95 reduction of 3.97% and 6.67% compared to LoCoOp and
LoRA.Moreover,whenscaleduptoCLIP-large,SeTAR+FToutperformsLoCoOpandLoRAby
relatively 17.92% and 12.45% FPR95 on the
Table 2: Fine-tuning results on ImageNet1K
samebenchmark. Similarresultsareobserved
benchmark. Boldvaluesindicatethehighestper-
onSwinTransformer(Liuetal.,2021),where formance. ‚Ä† is cited from Tao et al. (2023). ‚àó
SeTAR+FT outperforms LoRA by relatively
denotesourre-runresults,¬±indicatesthestandard
17.36% and 36.80% FPR95 on MSP and En-
deviationfrom3runs.
ergyscoringfunctions,respectively. Thelarger
MCMScore GL-MCMScore
improvement on Swin Transformer may stem CLIP-base FPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚Üë
from its reliance on ImageNet training, mak- NPOS‚Ä† 42.20 90.43 36.86 90.37
ingitpronetooverfittingandweakeratOOD CoOp‚Ä† 44.81 90.03 36.58 90.25
detection.Ourmethodmitigatestheseissues,en- LoCoOp‚Ä† 40.17 91.53 33.52 92.14
LoCoOp‚àó 39.76¬±4.06 91.22¬±0.52 34.14¬±1.64 91.73¬±0.17
hancingSwin‚ÄôsgeneralizationtoOODinstances. LoRA‚àó 41.67¬±0.14 90.85¬±0.01 34.36¬±0.11 90.88¬±0.01
Theseresultsdemonstratetheeffectivenessand SeTAR+FT 38.77¬±0.22 91.55¬±0.01 32.19¬±0.20 92.31¬±0.05
scalabilityofSeTAR+FTinimprovingtheOOD CLIP-large MCMScore GL-MCMScore
FPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚Üë
detectionperformance.
LoCoOp‚àó 40.74¬±3.80 91.13¬±0.79 46.74¬±4.19 89.32¬±0.80
Furthermore,asshowninFigure6,SeTAR+FT LoRA‚àó 38.62¬±0.07 91.66¬±0.02 43.39¬±0.01 89.76¬±0.03
SeTAR+FT 34.75¬±0.55 92.86¬±0.15 37.05¬±0.59 91.83¬±0.12
demonstratesfasterconvergenceandlowerloss
MSPScore EnergyScore
thanLoRA,especiallyinOODloss,indicating Swin-base FPR95‚Üì AUROC‚Üë FPR95‚Üì AUROC‚Üë
thatSeTAR+FTismoreeffectiveinadaptingthe LoRA‚àó 57.02¬±0.03 80.49¬±0.01 62.17¬±0.02 72.80¬±0.00
pre-trainedweightstotheOODdetectiontask. SeTAR+FT 47.12¬±0.42 87.80¬±0.44 39.29¬±0.57 88.01¬±0.51
4.4 AblationStudy
Inthissection,weconductablationstudieswithCLIP-basetounderstandourdesignchoices.
Imagev.s. Textmodality Table3showsanablationstudyonthemodalityinvolvedinSeTAR.
Asshown,thevisionmodalityoutperformsthe
Table3: Ablationstudyonmodality.
textmodality,indicatingthevisionmodalityis
Vision Text Vision+Text
moredominantinenhancingthemodel‚Äôsperfor- Score
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
mance. Whenconsideringthevisionmodality
ImageNet1K
alone and the combined vision+text modality,
MCM 40.27 91.24 42.78 90.50 40.24 91.05
the latter either outperforms or achieves com- GL-MCM 32.97 91.60 35.82 90.55 33.12 91.32
parable average results to the former. Conse- Pascal-VOC
quently,wemakemodificationstoboththevi- MCM 33.19 93.45 33.47 93.42 32.46 93.74
GL-MCM 24.88 94.51 24.59 94.52 23.86 94.87
sionandtextmodalitiesinSeTARtoenhance
overallperformance.
Different Weight Types In this part, we Table4: ComparisonresultsofSeTARwithand
presentempiricalevidenceformodifyingW up. withoutconsideringprojectionmatrixW p.
We first compare the performance of SeTAR
with different types of weight matrix in each Score
Vanilla SeTARwWp SeTARw/oWp
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
Transformerlayer,includingW ,W ,W ,W ,
q k v o ImageNet1K
W upandW down. AsshowninFigure2andFig- MCM 43.09 90.74 41.79 90.74 40.24 91.05
ure 3 of Appendx F, the X-axis denotes the GL-MCM 35.29 90.86 34.30 91.24 33.12 91.32
numberofweightmatrixes(layers)thatwehave Pascal-VOC
searched, whiletheY-axisistheaverageAU- MCM 37.24 92.98 35.94 93.32 32.46 93.74
GL-MCM 29.44 93.88 23.34 94.82 23.86 94.87
ROC and FPR95. The results show that W
up
consistentlyoutperformsotherweightmatricesintermsofbothAUROCandFPR95. Inadditionto
weightmatricsineachtransformerlayer,CLIPhasoneprojectionmatrixW ontopofeachencoder,
p
7
whichservestoprojectimage/textrepresentationsintoasharedspace. InTable4,wecomparethe
performanceofSeTARwithandwithoutmodifyingW . WesearchW firstrightbeforesearching
p p
the image/text encoder. The results show that frozen W brings a relatively reduction of 4.20%
p
FPR95. Consequently,wekeepW frozeninSeTAR.
p
DifferentSearchAlgorithms Ateachstepofthegreedysearch,SeTARtraversesthesubsequent
W in a predefined order and searches over different thresholds. We compare our method with
up
two alternatives: modality-interleaved greedy search (MIS) and layer-exhaustive search (LES).
MIS searches the image and text layers in an interleaved manner, while LES simultaneously
searches over both layers and thresholds at each step. SeTAR-S, has linear complexity with re-
spect to the number of model layers, similar to MIS, whereas LES has quadratic complexity.
Table5presentsthecomparisonresults. SeTAR-
Table 5: Results for different search algo-
Sdemonstratesbetteroverallperformancethan
rithms. Here LES, MIS and SeTAR-S stand
MIS.Notably,MISencounterslimitationswhen
for layer-exhaustive search, modality-interleave
theimageandtexttowershavedifferentlayer
greedysearch,andthesearchalgorithmofSeTAR.
counts (e.g., CLIP-large with 24 image lay-
LES MIS SeTAR-S
ers and 12 text layers). Therefore, we choose Score
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
SeTAR-Sforbettergeneralization. Compared
ImageNet1K
to LES, SeTAR-S performs better in terms of
MCM 41.99 90.78 40.55 91.00 40.24 91.05
bothFPR95andAUROC,asLES‚Äôslocallyopti- GL-MCM 33.90 91.08 33.36 91.29 33.12 91.32
malalgorithmmaynotachieveaglobaloptimal Pascal-VOC
solution. Theseresultsvalidatethesuperiority MCM 35.11 93.60 33.93 93.58 32.46 93.74
GL-MCM 24.48 94.57 22.87 94.84 23.86 94.87
ofourtop-to-bottomlayersearchstrategy.
Different Prune Strategies Inspired from SVD, SeTAR modify the model weights by prun-
ing the minor singular components, and retains the principle components that contribute the
most to the model‚Äôs performance. To validate this design, we compare SeTAR with two alter-
natives: principal component pruning and random pruning pruning. Principal component takes
theoppositeapproach,retainingminorcompo-
Table6: Resultsfordifferentpruningstrategies.
nentsandpruningmajorones. Randompruning,
Principle Random Minor
ontheotherhand,prunesweightsrandomly. As Score
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
showninTable6,principlepruningsuffersfrom
ImageNet1K
asignificantperformancedropcomparedtoSe-
MCM 43.09 90.74 43.09 90.74 40.24 91.05
TAR,whilerandompruningperformsslightly GL-MCM 35.29 90.86 35.29 90.86 33.12 91.32
better than principle pruning. These results Pascal-VOC
demonstratetheeffectivenessofSeTAR‚Äôsde- MCM 38.20 92.44 33.57 93.09 32.46 93.74
GL-MCM 25.36 93.67 26.20 94.66 23.86 94.87
signchoiceinpruningtheminorcomponents.
Sensitivity Analysis on Œª and top-K In this section, we present the sensitivity analysis of the
hyperparametersŒª(Figure4)andtop-K(Figure5). AsobservedinFigure4,theaverageAUROC
remains stable at lower values and slightly decreases as Œª increases for both SeTAR+MCM and
SeTAR+GL-MCM.Notably,theoptimalsettingofŒªmayvarydependingonthemodelbackbone,
withourexperimentsindicatingthatCLIP-largemayrequirealargerŒªthanCLIP-base. Despitethis
variation,theŒªparameterdemonstratesstrongtransferabilityacrossdatasetsforthesamebackbone.
SwappingtheoptimalŒªbetweenImageNet1KandPascal-VOChasaminimalperformanceimpact,
consistentlyoutperformingthevanillamethod. WiththeVOC-optimizedŒªonImageNet1K,CLIP-
baseachievesanFPR95of40.91andAUROCof91.02,andCLIP-largereaches46.73FPR95and
91.81AUROC.Conversely,usingtheImageNet1K-optimizedŒªonPascal-VOC,CLIP-baseachieves
33.18FPR95and93.65AUROC,whileCLIP-largeattains44.39FPR95and92.3AUROC.
Top-KcontrolsthenumberofOODregionsconsideredinLoCoOploss: highervaluesincludemore
OODregions,withtop-KequaltothenumberofIDclassescoveringallOODregions,andtop-Kset
to0focusingsolelyonIDloss. Theoptimaltop-KdependsonthenumberofIDcategories,making
itnon-transferableacrossdatasets. However,SeTARremainsrobusttotop-Kvariations,asshownin
Figure5,exceptatextremevalues(0orthemaximumnumberofclasses). Werecommendsetting
top-Ktoaround30%ofthetotalcategories,suchas300forImageNet1Kand4forPascal-VOC.For
theSwin-basemodel,top-Kat300onImageNet1KyieldsanFPR95of56.82andAUROCof85.68
withMSP,andanFPR95of52.56andAUROCof84.51withEnergy.
8
4.5 Analyses
Can SeTAR Improve Image Classification? Table7: Imageclassificationresultswithdiffer-
To evaluate the impact of SeTAR and Se- entmethods. WeuseImageNet1K(IN1K)asID
TAR+FTonclassificationaccuracy,wepresent dataset. ‚àó denotes the results of our re-run. The
ourresultsonIDdatasetImageNet1KandOOD resultsareaveragedover3runs.
datasets SUN, Places and Texture in Table 77.
Method IN1K SUN Places Texture Average
SeTAR effectively maintains the average ac-
VanillaCLIP‚àó 64.07 75.77 45.65 43.60 57.27
curacy, withminorvariationsobservedacross LoCoOp‚àó 64.93 75.89 46.47 37.79 56.27
differentdatasets. Amongthefine-tunedbase- LoRA‚àó 65.43 76.86 46.58 43.98 58.21
SeTAR 63.97 75.50 45.81 43.76 57.26
lines,LoCoOpexhibitsa1%decreaseinaccu-
SeTAR+FT 67.02 77.94 46.64 43.28 58.72
racycomparedtoVanillaCLIP,whereasLoRA
shows an improvement of 0.94%. Notably, SeTAR+FT surpasses both baselines, improving the
averageaccuracyby1.45%comparedtoVanillaCLIP.TheseresultshighlighttheefficacyofSeTAR
andSeTAR+FTinimprovingOODdetectionwithoutcompromisingclassificationaccuracy.
SeTAR is Effective on Different Architectures and Score Functions We expand on Table 1
withresultsonViTandCNNbackbonesandvariousscorefunctions. ForViT-basedmodels,we
evaluate OOD detection using CLIP-large8 and Swin Transformer9 (Liu et al., 2021), alongside
CLIP-base. The Swin Transformer (Liu et al., 2022) is trained on ImageNet1K. Since it lacks a
textencoder,weapplySeTARtotheimageViTonly. ForSwinTransformer,weusetwocommon
scoringfunctions: MSP(Hendrycks&Gimpel,2017), whichleveragessoftmaxconfidence, and
theEnergyscore(Liuetal.,2020),withT =0.1forOODdetection. WealsointegrateCLIP-base
withtheNegLabelscorefunction(Jiangetal.,
Table8: ResultsfordifferentViTbackbones.
2024), which uses large-scale negative labels.
VanillaMethod SeTAR
AsshowninTable8,SeTARconsistentlyoutper- Backbone Score
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
formsbaselinesacrossallbackbonesandscor-
ingfunctions,significantlyreducingFPR95by ImageNet1K
CLIP-base NegLabel 25.40 94.21 23.09 94.48
relatively20.61%withtheEnergyscoreonSwin
CLIP-large MCM 37.19 91.73 36.26 91.92
Transformer. TheseresultsdemonstrateSeTAR CLIP-large GL-MCM 40.65 89.98 39.54 90.22
‚ÄôseffectivenessinimprovingOODdetectionfor Swin-base MSP 59.25 84.12 56.05 85.77
Swin-base Energy 65.01 76.10 51.61 84.42
unimodal image encoders, with further confir-
mationfromSeTAR+FTresults(Table2)across Pascal-VOC
CLIP-large MCM 52.21 91.68 42.57 92.91
differentmodelbackbones. CLIP-large GL-MCM 43.96 92.45 31.12 94.00
WefurtherexploreSeTAR‚ÄôspotentialonCNN
Table 9: Results on ResNet50. We use Ima-
architecture,andcompareitwithmethodssuch geNet1KastheIDdataset. ‚Ä†iscitedfromDjurisic
as Softmax, Energy (Wu et al., 2023), Re-
etal.(2023).
Act(Sunetal.,2021),DICE(Sun&Li,2022),
Method FPR‚Üì AUC‚Üë Method FPR‚Üì AUC‚Üë
andASH(Djurisicetal.,2023)onResNet5010.
SinceResNetlackslocalfeaturesforOODloss, Softmax‚Ä† 66.95 81.99 ASH-P‚Ä† 50.32 89.04
Energy‚Ä† 58.41 86.17 ASH-B‚Ä† 22.73 95.06
weconductexperimentsusingonlyIDloss. We
ReAct‚Ä† 31.43 92.95 ASH-S‚Ä† 22.80 95.12
applylow-rankapproximationtothein-andout-
DICE‚Ä† 34.75 90.77 SeTAR 22.38 95.25
featuredimensionsoftheconvolutionallayers,
combinedwithASHforsearch. AsshowninTable9,SeTARestablishesnewstate-of-the-artresults
onResNet,demonstratingitseffectivenessacrossbothViTandCNNarchitectures.
Near-OOD Results To further evaluate Se- Table10: Near-OODresultsonCLIP-base.
TAR‚ÄôsperformanceondiverseOODtasks,we
MCMScore GL-MCMScore
Method Category
testitonamorechallengingnear-OODsetting
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
usingImageNet1KastheIDdatasetandSSB-
Vanilla Training-Free 89.28 63.88 85.62 67.63
Hard(Vazeetal.,2022)astheOODdataset. As SeTAR Training-Free 88.29 64.20 84.03 68.29
showninTable10,SeTARandSeTAR+FTout- LoCoOp Training-Free 89.72 63.45 86.79 65.93
LoRA Finetuning 88.52 65.38 84.39 68.85
performthebaselines, demonstratingsuperior SeTAR+FT Finetuning 87.16 68.13 84.72 70.42
performanceinnear-OODscenarios.
7WedonotreportclassificationaccuracyoniNaturalistaswefailedtomatchthelabelsfortheOODtestset.
8https://huggingface.co/openai/clip-vit-large-patch14
9https://huggingface.co/microsoft/swinv2-base-patch4-window16-256
10https://download.pytorch.org/models/resnet50-19c8e357.pth
9
5 RelatedWork
Out-of-DistributionDetection PreviousworkexploresOODdetectionwithunimodal(DeVries
& Taylor, 2018; Hendrycks & Gimpel, 2017; Hu & Khan, 2021; Zheng et al., 2020; Zhou et al.,
2021)andmultimodal(Fortetal.,2021;Mingetal.,2022;Taoetal.,2023;Miyaietal.,2023a)
models. Numerous methodologies (Lee et al., 2018; Huang et al., 2021; Sun et al., 2022; Wang
et al., 2022; Wu et al., 2023) have been developed to tackle OOD detection in computer vision.
ExistingCLIP-basedOODdetectionmethodsincludezero-shot(Fortetal.,2021;Mingetal.,2022;
Miyaietal.,2023b;Daietal.,2023;Wangetal.,2023;Jiangetal.,2024)andfine-tuning(Ming&
Li,2023;Taoetal.,2023;Miyaietal.,2023a). Zero-shotmethodslikeMCM(Mingetal.,2022)
and GL-MCM (Miyai et al., 2023b) don‚Äôt require in-distribution training data but may perform
suboptimallyduetodomaingaps. Otherapproachesintegrateexternalknowledge. Forexample,
CLIPN(Wangetal.,2023)pre-trainsanovelNO-encoderontheCC-3Mdataset(Sharmaetal.,2018)
toempowerCLIP‚Äôs"no"logicforzero-shotevaluation. NegLabel(Jiangetal.,2024)demonstrates
betterperformancethanCLIPNbyintroducinglarge-scalenegativelabelsforenhancedlabelscoring.
Fine-tuningmethods(Ming&Li,2023;Taoetal.,2023;Miyaietal.,2023a)improveOODdetection
byadaptingtoin-distributiondatabutriskdamagingthepretrainingrepresentations,needingcareful
trainingstrategies. CNN-basedOODdetectionmethods,includingReAct(Sunetal.,2021),ASH
(Djurisicetal.,2023),DICE(Sun&Li,2022),CIDER(Mingetal.,2023),PALM(Luetal.,2024),
and Hopfield Boosting (Hofmann et al., 2024), have also demonstrated strong results. However,
methods like ReAct and ASH rely on the assumption that ID and OOD images produce distinct
activationsinmodelstrainedonIDdata. Thisassumptiondoesnotholdinlarge-scalepre-trained
modelslikeCLIP,whereactivationsforIDandOODimagesarenotsignificantlydifferent,limiting
the effectiveness of such approaches in enhancing CLIP‚Äôs zero-shot OOD detection capabilities.
SeTAR,incontrast,offershighcompatibilitywithvariousscoringfunctions(e.g.,MCM,GL-MCM,
MSP,Energy),multiplemodelbackbones(e.g.,CLIP,Swin,ResNet),andadvancedOODtechniques
suchasNegLabel. Designedtobebothlightweightandefficient,SeTARaddressesthedemandfor
resource-efficientsolutionsinOODdetection.
Low-rank Approximations of Weight Matrices Neural networks trained with over-
parameterization often exhibit low-rank properties (Oymak et al., 2019). These properties are
utilizedinbothmodeltraining(Poveyetal.,2018;Huetal.,2022)andpost-hocprocessing(Haji-
molahoseinietal.,2021;Sharmaetal.,2023). Intraining,someworks(Sainathetal.,2013;Zhang
etal.,2014;Zhaoetal.,2016)imposelow-rankconstraints,whileLoRA(Huetal.,2022)adapts
pretrainedLLMstodownstreamtasksusingtrainablelow-rankmatrices. Forpost-hocprocessing,
pruningmethods(Yuetal.,2017;Hajimolahoseinietal.,2021)reduceweightmatrixranksbyretain-
ingtop-KcomponentsfromSVD.Whilepruningpreservesmodelbehavior,performancedeclines
withincreasedintervention. LASER(Sharmaetal.,2023)focusesonpruningindividuallayersto
enhancefactualansweringcapabilities. Itutilizesasimplegreedysearchstrategyonavalidation
set,whichisnotapplicableforOODdetectionduetotheabsenceofavalidationset. Incontrast,our
approachintroducesaselectiverankreductionstrategyspecificallytailoredforOODdetection. We
systematicallyanalyzeandcomparedifferentgreedysearchtechniques,evaluatingtheireffectiveness
acrossvariouslayersandmodelbackbones.
6 Conclusion
We propose SeTAR , a simple and effective OOD detection method using post-hoc low-rank ap-
proximationonweightmatricesW withatop-down,image-to-textgreedysearch. SeTARoffers
up
several advantages: (1) training-free, (2) scalable to unimodal and multimodal models, and (3)
complementarytoexistingOODscoringfunctions. BuildingonSeTAR,weintroduceSeTAR-FT,
afinetuningmethodthatadaptsthemodeltoin-distributiondataforimprovedOODdetection. We
evaluateSeTARandSeTAR-FTonlarge-scalebenchmarks,includingImageNet1KandPascal-VOC.
Results show that both achieve state-of-the-art OOD detection performance. We hope our work
inspiresfurtherresearchandcontributestomorerobustandreliablemodels.
10
Acknowledgements
ThisprojectwassupportedbyNationalNaturalScienceFoundationofChina(No. 62306132,No.
62106138). Wethanktheanonymousreviewersfortheirinsightfulfeedbacksonthiswork.
References
Bai, H., Canal, G., Du, X., Kwon, J., Nowak, R. D., and Li, Y. Feed two birds with one scone:
Exploitingwilddataforbothout-of-distributiongeneralizationanddetection. InInternational
ConferenceonMachineLearning,2023.
Caesar,H.,Uijlings,J.,andFerrari,V. Coco-stuff: Thingandstuffclassesincontext. InCVPR,2018.
Cimpoi,M.,Maji,S.,Kokkinos,I.,Mohamed,S.,andVedaldi,A. Describingtexturesinthewild. In
CVPR,2014.
Dai, Y., Lang, H., Zeng, K., Huang, F., and Li, Y. Exploring large language models for
multi-modalout-of-distributiondetection. ArXiv, abs/2310.08027, 2023. URLhttps://api.
semanticscholar.org/CorpusID:263909127.
Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei,L. Imagenet: Alarge-scalehierarchical
imagedatabase. InCVPR,2009.
DeVries,T.andTaylor,G.W.Learningconfidenceforout-of-distributiondetectioninneuralnetworks.
arXivpreprint:1802.04865,2018.
Djurisic, A., Bozanic, N., Ashok, A., and Liu, R. Extremely simple activation shaping for out-
of-distributiondetection. InTheEleventhInternationalCosun2021reactnferenceonLearning
Representations,2023. URLhttps://openreview.net/forum?id=ndYXTEL6cZz.
Emmott, A., Das, S., Dietterich, T., Fern, A., andWong, W.-K. Ameta-analysisoftheanomaly
detectionproblem. arXivpreprint:1503.01158,2016.
Everingham,M.,VanGool,L.,Williams,C.K.,Winn,J.,andZisserman,A. Thepascalvisualobject
classes(voc)challenge. IJCV,88:303‚Äì308,2009.
Fort,S.,Ren,J.,andLakshminarayanan,B. Exploringthelimitsofout-of-distributiondetection. In
NeurIPS,2021.
Hajimolahoseini,H.,Rezagholizadeh,M.,Partovinia,V.,Tahaei,M.S.,Awad,O.M.,andLiu,Y.
Compressingpre-trainedlanguagemodelsusingprogressivelowrankdecomposition. InNeurIPS,
2021.
Hendrycks,D.andGimpel,K.Abaselinefordetectingmisclassifiedandout-of-distributionexamples
inneuralnetworks. InICLR,2017.
Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan, R., and Song, D. Pretrained trans-
formersimproveout-of-distributionrobustness. InProceedingsofthe58thAnnualMeetingofthe
AssociationforComputationalLinguistics,pp.2744‚Äì2751,Online,July2020.
Hendrycks, D., Basart, S., Mazeika, M., Mostajabi, M., Steinhardt, J., and Song, D. Scaling
out-of-distributiondetectionforreal-worldsettings. InICML,2022.
Hofmann,C.,Schmid,S.,Lehner,B.,Klotz,D.,andHochreiter,S. Energy-basedhopfieldboosting
forout-of-distributiondetection,2024. URLhttps://arxiv.org/abs/2405.08766.
Hu,E.J.,yelongshen,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,S.,Wang,L.,andChen,W. LoRA:
Low-rankadaptationoflargelanguagemodels. InICLR,2022.
Hu,Y.andKhan,L. Uncertainty-awarereliabletextclassification. InProceedingsofthe27thACM
SIGKDDConferenceonKnowledgeDiscovery&DataMining,pp.628‚Äì636,NewYork,NY,USA,
2021.
11
Huang,R.,Geng,A.,andLi,Y. Ontheimportanceofgradientsfordetectingdistributionalshiftsin
thewild. InNeurIPS,2021.
Jiang,X.,Liu,F.,Fang,Z.,Chen,H.,Liu,T.,Zheng,F.,andHan,B. NegativelabelguidedOOD
detectionwithpretrainedvision-languagemodels. InTheTwelfthInternationalConferenceon
LearningRepresentations,2024. URLhttps://openreview.net/forum?id=xUO1HXz4an.
Lee,K.,Lee,K.,Lee,H.,andShin,J. Asimpleunifiedframeworkfordetectingout-of-distribution
samplesandadversarialattacks. InNeurIPS,2018.
Li, J., Li, D., Savarese, S., andHoi, S. BLIP-2: bootstrappinglanguage-imagepre-trainingwith
frozen image encoders and large language models. In Proceedings of the 40th International
ConferenceonMachineLearning,2023.
Liang,S.,Li,Y.,andSrikant,R. Enhancingthereliabilityofout-of-distributionimagedetectionin
neuralnetworks. InICLR,2018.
Lin,T.-Y.,Maire,M.,Belongie,S.,Hays,J.,Perona,P.,Ramanan,D.,Doll√°r,P.,andZitnick,C.L.
Microsoftcoco: Commonobjectsincontext. InECCV,2014.
Liu,W.,Wang,X.,Owens,J.,andLi,Y. Energy-basedout-of-distributiondetection. InNeurIPS,
2020.
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer:
Hierarchicalvisiontransformerusingshiftedwindows. InICCV,2021.
Liu,Z.,Hu,H.,Lin,Y.,Yao,Z.,Xie,Z.,Wei,Y.,Ning,J.,Cao,Y.,Zhang,Z.,Dong,L.,Wei,F.,and
Guo,B. Swintransformerv2: Scalingupcapacityandresolution,2022.
Low-RankApproximation. Low-rankapproximation‚ÄîWikipedia,thefreeencyclopedia,January
2024. https://en.wikipedia.org/w/index.php?title=Low-rank_approximation&
oldid=1196167027.
Lu,H.,Gong,D.,Wang,S.,Xue,J.,Yao,L.,andMoore,K. Learningwithmixtureofprototypesfor
out-of-distributiondetection.InTheTwelfthInternationalConferenceonLearningRepresentations,
2024. URLhttps://openreview.net/forum?id=uNkKaD3MCs.
Ming, Y. and Li, Y. How does fine-tuning impact out-of-distribution detection for vision-
languagemodels? InternationalJournalofComputerVision,132(2):596‚Äì609,September2023.
ISSN 1573-1405. doi: 10.1007/s11263-023-01895-7. URL http://dx.doi.org/10.1007/
s11263-023-01895-7.
Ming,Y.,Cai,Z.,Gu,J.,Sun,Y.,Li,W.,andLi,Y. Delvingintoout-of-distributiondetectionwith
vision-languagerepresentations. InNeurIPS,2022.
Ming,Y.,Sun,Y.,Dia,O.,andLi,Y.Howtoexploithypersphericalembeddingsforout-of-distribution
detection? InTheEleventhInternationalConferenceonLearningRepresentations,2023. URL
https://openreview.net/forum?id=aEFaE0W5pAd.
Miyai,A.,Yu,Q.,Irie,G.,andAizawa,K. Locoop:Few-shotout-of-distributiondetectionviaprompt
learning. InThirty-SeventhConferenceonNeuralInformationProcessingSystems,2023a.
Miyai,A.,Yu,Q.,Irie,G.,andAizawa,K. Zero-shotin-distributiondetectioninmulti-objectsettings
usingvision-languagefoundationmodels. arXivpreprintarXiv:2304.04521,2023b.
Oymak,S.,Fabian,Z.,Li,M.,andSoltanolkotabi,M. Generalizationguaranteesforneuralnetworks
viaharnessingthelow-rankstructureofthejacobian,2019.
Povey,D.,Cheng,G.,Wang,Y.,Li,K.,Xu,H.,Yarmohammadi,M.A.,andKhudanpur,S. Semi-
orthogonallow-rankmatrixfactorizationfordeepneuralnetworks. InInterspeech,2018. URL
https://api.semanticscholar.org/CorpusID:4949673.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,
Mishkin,P.,Clark,J.,etal. Learningtransferablevisualmodelsfromnaturallanguagesupervision.
InICML,2021.
12
Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,A.,Khosla,
A.,Bernstein,M.,etal. Imagenetlargescalevisualrecognitionchallenge. IJCV,115(3):211‚Äì252,
2015.
Sainath,T.N.,Kingsbury,B.,Sindhwani,V.,Arisoy,E.,andRamabhadran,B. Low-rankmatrix
factorizationfordeepneuralnetworktrainingwithhigh-dimensionaloutputtargets. In2013IEEE
internationalconferenceonacoustics,speechandsignalprocessing,pp.6655‚Äì6659.IEEE,2013.
Sharma,P.,Ding,N.,Goodman,S.,andSoricut,R. Conceptualcaptions: Acleaned,hypernymed,
imagealt-textdatasetforautomaticimagecaptioning. InProceedingsofACL,2018.
Sharma,P.,Ash,J.T.,andMisra,D. Thetruthisinthere: Improvingreasoninginlanguagemodels
withlayer-selectiverankreduction,2023.
Sun,Y.andLi,Y. Dice: Leveragingsparsificationforout-of-distributiondetection. InComputerVi-
sion‚ÄìECCV2022:17thEuropeanConference,TelAviv,Israel,October23‚Äì27,2022,Proceedings,
PartXXIV,pp.691‚Äì708,Berlin,Heidelberg,2022.Springer-Verlag.ISBN978-3-031-20052-6.doi:
10.1007/978-3-031-20053-3_40. URL https://doi.org/10.1007/978-3-031-20053-3_
40.
Sun,Y.,Guo,C.,andLi,Y.React:Out-of-distributiondetectionwithrectifiedactivations.InNeurIPS,
2021.
Sun,Y.,Ming,Y.,Zhu,X.,andLi,Y. Out-of-distributiondetectionwithdeepnearestneighbors. In
ICML,2022.
Tao,L.,Du,X.,Zhu,X.,andLi,Y. Non-parametricoutliersynthesis. InICLR,2023.
VanHorn,G.,MacAodha,O.,Song,Y.,Cui,Y.,Sun,C.,Shepard,A.,Adam,H.,Perona,P.,and
Belongie,S. Theinaturalistspeciesclassificationanddetectiondataset. InCVPR,2018.
Vaze,S.,Han,K.,Vedaldi,A.,andZisserman,A. Open-setrecognition: Agoodclosed-setclassifier
isallyouneed. InICLR,2022.
Wang,H.,Liu,W.,Bocchieri,A.,andLi,Y. Canmulti-labelclassificationnetworksknowwhatthey
don‚Äôtknow? InNeurIPS,2021.
Wang,H.,Li,Z.,Feng,L.,andZhang,W. Vim: Out-of-distributionwithvirtual-logitmatching. In
CVPR,2022.
Wang,H.,Li,Y.,Yao,H.,andLi,X. Clipnforzero-shotooddetection: Teachingcliptosayno. 2023
IEEE/CVFInternationalConferenceonComputerVision(ICCV),pp.1802‚Äì1812,2023. URL
https://api.semanticscholar.org/CorpusID:261076240.
Wu, Q., Chen, Y., Yang, C., and Yan, J. Energy-based out-of-distribution detection for graph
neuralnetworks. ArXiv,abs/2302.02914,2023. URLhttps://api.semanticscholar.org/
CorpusID:256615740.
Xiao, J., Hays, J., Ehinger, K. A., Oliva, A., and Torralba, A. Sun database: Large-scale scene
recognitionfromabbeytozoo. InCVPR,2010.
Xu, K., Ren, T., Zhang, S., Feng, Y., and Xiong, C. Unsupervised out-of-domain detection via
pre-trainedtransformers. InACL,2021.
Yao,X.,Hu,X.,Yang,S.,andLiu,Y. Enhancingin-contextlearningperformancewithjustsvd-based
weightpruning: Atheoreticalperspective,2024. URLhttps://arxiv.org/abs/2406.03768.
Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. CoCa: Contrastive
captionersareimage-textfoundationmodels. arXivpreprintarXiv:2205.01917,2022.
Yu, X., Liu, T., Wang, X., and Tao, D. On compressing deep models by low rank and sparse
decomposition. 2017IEEEConferenceonComputerVisionandPatternRecognition(CVPR),pp.
67‚Äì76,2017. URLhttps://api.semanticscholar.org/CorpusID:24553488.
13
Zhang,Y.,Chuangsuwanich,E.,andGlass,J.R. Extractingdeepneuralnetworkbottleneckfeatures
usinglow-rankmatrixfactorization. 2014IEEEInternationalConferenceonAcoustics,Speech
andSignalProcessing(ICASSP),pp.185‚Äì189,2014. URLhttps://api.semanticscholar.
org/CorpusID:1791734.
Zhao,Y.,Li,J.,andGong,Y.Low-rankplusdiagonaladaptationfordeepneuralnetworks.2016IEEE
InternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.5005‚Äì5009,
2016. URLhttps://api.semanticscholar.org/CorpusID:10506309.
Zheng,Y.,Chen,G.,andHuang,M. Out-of-domaindetectionfornaturallanguageunderstanding
in dialog systems. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:
1198‚Äì1209,2020.
Zhou,B.,Lapedriza,A.,Khosla,A.,Oliva,A.,andTorralba,A. Places: A10millionimagedatabase
forscenerecognition. TPAMI,40(6):1452‚Äì1464,2017.
Zhou,K.,Yang,J.,Loy,C.C.,andLiu,Z. Learningtopromptforvision-languagemodels. IJCV,
2022.
Zhou,W.,Liu,F.,andChen,M. Contrastiveout-of-distributiondetectionforpretrainedtransformers.
InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
pp.1100‚Äì1111,OnlineandPuntaCana,DominicanRepublic,November2021.
14
A ImpactStatements
Limitation WhilewedemonstratetheeffectivenessofourmethodonOODdetection,weacknowl-
edgethatourworkhasseverallimitations. First,despiteweshowtherobustnessofourmethodto
hyperparameters,theoptimalhyperparametersmayvaryacrossdifferentmodelbackbones. Future
work is needed to explore the autonomous selection of hyperparameters. Second, we design Se-
TAR+FTinasimpleandstraightforwardmanner,whichmaynotbethemostefficientoreffectiveway
toadaptthemodeltotheIDdownstreamdata. Moresophisticatedstrategiesformodeladaptationare
worthexploringinfutureresearch. Third,weonlyconductexperimentstodetectvisualOODinputs
andignoreinputsinothermodalitiessuchastextual, audioandvideo. Thisisprimarilybecause
ourmodelisbasedonCLIP.ExploringthedevelopmentofOODdetectorsacrossdiversemodalities
remainsanactiveresearchtopicforfutureinvestigation.
EthicalConsiderations OurstudyaddressesthechallengeofOODdetectionthroughlow-rank
approximation, which is particularly relevant for ensuring the reliability and trustworthiness of
vision-and-languagepre-trainedmodels. Futureinvestigationsonfairness,privacyandtransparency
neural-basedmodelsshouldbeencouragedtomitigatetheexistingdatabiasesandsafetyproblems
foraresponsible,helpfulandtrustworthyAIsystemindiversereal-worldapplications.
FutureSocietalConsequences OurproposedSeTARachievesimpressiveOODdetectionperfor-
mance,whichisbeneficialtovariousreal-worldmachinelearningapplications,suchashealthcare
and autonomous vehicles. The identification of anomalies or unexpected data points is crucial
fordecision-makingandriskmanagementwithAImodels. AbetterOODdetectorfacilitatesthe
developmentoftrustworthymachine-learningmodelsthatcanrejectunknowndatainputsandhelp
alleviatethehallucinationproblem. Moreover,betterOODdetectorslikeSeTARcanhelptoselect
andlabeltheunfamiliardatasamplestofurthertrainastrongermodelinthewild.
B LossFunction
Toimprovethemodel‚ÄôsOODdetectionability,itiscrucialtodefinealossfunctionthatpushesOOD
samplesfarfromIDsampleswhilekeepingIDsamplesclosetoeachother. However,sinceOOD
samplesareunavailableduringdevelopment,weaddressthisissuebyusingtheLoCoOploss(Miyai
etal.,2023a)forbothSeTARandSeTAR+FT.ThemainideaistocreatepseudoOODfeatureswith
ID-irrelevantnuisances(e.g.,backgrounds)inCLIP‚Äôslocalfeatures.
Specifically, we divide the image into patches, represented by the set of all patch indices I =
{0,1,2,...,H √óW ‚àí1},whereH andW denotetheheightandwidthofthepatchfeatures. Next,
wecomputethecosinesimilaritybetweentheimagepatchfeaturespv andthetextfeaturesht ofthe
i c
imagelabel. Theclassificationpredictionprobabilitiesforeachpatchiarethengivenby:
exp(cos_sim(pv,ht)/œÑ‚Ä≤)
p (y =m|x)= i c (9)
i (cid:80)K exp(cos_sim(pv,ht)/œÑ‚Ä≤)
c=1 i c
ForagivenimagepatchrelatedtoanIDcategory,thecorrespondingIDlabelshouldbeamongits
top-Kpredictions. Conversely,forpatchesunrelatedtotheIDlabel,suchasbackgroundregions,
theIDlabelshouldbeexcludedfromthetop-Kpredictions. Basedonthisintuition,theindicesof
ID-irrelevantregionswithinanimagearedefinedbyEquation10,whererank(p (y =y|x))denotes
i
therankofthetrueclassyamongallIDclasses,andKisthehyperparameter.
J ={i|rank(p (y =y|x))>K} (10)
i
After identifying out-of-distribution (OOD) regions, it is expected that their image features will
differsignificantlyfromtheIDtextembeddings. Toenhancethisdistinction,entropymaximization
is employed to increase the entropy of p (y|x), where p denotes the classification prediction
j j
probabilitiesforregionj ‚ààJ. Theentropymaximizationisformallydefinedasfollows:
L =‚àíH(p ) (11)
ood j
Here,H(¬∑)representstheentropyfunction. TheoveralllossfunctioncombinestheIDloss(cross-
entropylossforIDpredictions)withtheOODloss. HereŒªisthehyperparameterthatregulatesthe
proportionoftheOODloss.
L=L +ŒªL (12)
id ood
15
C Data
Table11: Thestatisticsofthedatasetusedinthispaper. ‚ÄòID‚Äôand‚ÄòOOD‚Äôdenotein-distribution
andout-of-distribution,respectively.
Data Type ValidSize TestSize
ImageNet1K(Dengetal.,2009) ID 1,000 50,000
Pascal-VOC(Everinghametal.,2009) ID 94 906
iNaturalist(VanHornetal.,2018) OOD 0 10,000
SUN(Xiaoetal.,2010) OOD 0 10,000
Places(Zhouetal.,2017) OOD 0 10,000
Texture(Cimpoietal.,2014) OOD 0 5,640
ImageNet22K(Russakovskyetal.,2015) OOD 0 18,335
COCO(Linetal.,2014) OOD 0 1,000
We use two real-world datasets created from ImageNet1K (Deng et al., 2009) and Pascal-VOC
(Everinghametal.,2009)astheIDdataset. WeuseImageNet-1KvalidationsetastheIDtestset
followingMingetal.(2022),andpreprocessPascal-VOCfollowingMiyaietal.(2023b). webuild
twoIDvalidationsetsforlow-rankapproximation. TheIDvalidationsetofImageNet1Kiscollected
by sampling one image for each label from the ImageNet1K training set. For Pascal-VOC, For
Pascal-VOC,Werandomlysample10%imagesastheIDvalidationsetandleavetherestastheID
testset.
ForOODdatasets,wefollowMingetal.(2022)topreprocessiNaturalist,SUN,PlacesandTexture,
andfollowMiyaietal.(2023b)topreprocessImageNet22KandCOCOdata. Weonlyevaluatethe
OODdatasetsthathavenooverlappingcategoriesastheIDdataset.
We provide more details about the datasets used in our experiments, in terms of data sources,
preprocessing,andthestatisticsforeachdataset,asshowninTable11andbelow.
ImageNet1K WeusetheImageNet-1000(ILSVRC2012)(Dengetal.,2009)datasetforIDvali-
dationandtesting. Theoriginaldatasetcontains1.2milliontrainingimagesand50,000validation
imagesfrom1000classes,andiswidelyusedforimageclassification.WefollowMingetal.(2022)to
constructtheImageNet1KIDtestsetfromthevalidationset. Additionally,wecurateanImageNet1K
IDvalidationsetfromthetrainingsetbyrandomlyselectingoneimageforeachlabel.
Pascal-VOC The Pascal VOC (Visual Object Classes) (Everingham et al., 2009) dataset is a
benchmarkdatasetwidelyusedincomputervision,featuringannotatedimagesacrossmultipleobject
categories. WeusethePascal-VOCsubsetcollectedbyMiyaietal.(2023b)astheIDdataset,each
imagehassingle-classIDobjectsandoneormoreOODobjects. TheIDvalidationandtestsetare
splitby1:9foreachclass,resultingin94and906images,respectively.
iNaturalist iNaturalist (Van Horn et al., 2018) is a biodiversity dataset containing millions of
labeledimagesofplants,animals,andinsects. Mingetal.(2022)constructasubsetwith10,000
imagesbyde-duplicatingconceptsoverlappedwithIDdatasets.
Places Places(Zhouetal.,2017)isascene-centricdatabasewith205scenecategoriesand2.5
millionimages. WeusetheSUNsubsetcollectedbyMingetal.(2022)astheOODtestset,which
contains10,000imagesthatarenotoverlappedwiththeIDclasses.
SUN SUN(SceneUNderstanding)(Xiaoetal.,2010)isacomprehensivecollectionoflabeled
imagesrepresentingadiverserangeofindoorandoutdoorscenes. WeusetheSUNsubsetcollected
byMingetal.(2022)astheOODtestset,whichcontains10,000imagesthatarenotoverlappedwith
theIDclasses.
Texture The Texture dataset (DTD) (Cimpoi et al., 2014) comprises 5640 images categorized
into47termsinspiredbyhumanperception,aimedatreplicatinghuman-liketexturerecognitionin
machines. Again,weusethesubsetcollectedbyMingetal.(2022)astheOODtestset.
ImageNet22K TheImageNet-22Kdataset(Russakovskyetal.,2015),formerlyknownasImageNet-
21K,addressestheunderestimationofitsadditionalvaluecomparedtothestandardImageNet-1K
16
pretraining,aimingtoprovidehigh-qualitypretrainingforabroaderrangeofmodels. Weusethe
filteredsubsetcollectedbyWangetal.(2021)astheOODtestsetforMC-COCOandPascal-VOC
IDtestsets.
COCO Miyaietal.(2023b)curatedanMS-COCOOODtestset(COCOforshort)with1,000
imagesthatarenotoverlappedwiththePascal-VOCIDclasses,whichweuseasOODtestingdata
forPascal-VOCIDtestset.
D Fine-tuneBaselines
WecompareSeTAR+FTwith4finetuning-basedbaselines. Thesebaselinesinclude:
‚Ä¢ NPOS.NPOS(Taoetal.,2023)generatesvirtualanomaliesinlow-probabilityregionsofID
datawithoutrelyingondistributionassumptions,enhancingdiscriminationduringtraining.
‚Ä¢ CoOp. CoOp (Zhou et al., 2022) optimizes prompts for vision-language models with
learnablecontextvectorsforefficientfew-shotlearning.
‚Ä¢ LoCoOp. LoCoOp(Miyaietal.,2023a)improvesuponCoOpbyleveragingCLIP‚Äôslocal
features to better distinguish between ID and OOD samples, achieving higher detection
accuracy with less training data. We follow the official code11 to prepare and fine-tune
theLoCoOpwithCLIP-baseandCLIP-large. FollowMiyaietal.(2023a),thetop-K,Œª,
learning rate and epoch num are set to 200, 0.25, 0.002 and 50. Temperature is set to 1
andthetextpromptisinitiatedwith‚ÄúX X X X X X X X X X X X X X X X [CLASS]‚Äù,
where[CLASS]istheIDclassname. Weaveragetheresultsfrom3seedsfinetunedwith
1-shotImageNet1Kvaliddata.
‚Ä¢ LoRA.LoRA(Huetal.,2022)isalow-rankadaptationmethodthatinjectstrainablelow-
rankdecompositionmatricesintothepre-trainedmodeltoadapttodownstreamtasks. We
applylow-rankadaptationtothesameweighttypeasSeTAR+FT,therankofeachlayeris
settomatchthetrainableparametersofSeTAR.DetailssettingscanbefoundinTable13.
E HyperParametersSettings
ThehyperparametersforSeTARareshowninTable12. AndthehyperparametersforSeTAR+FT
andLoRAareshowninTable13.
Table12: HyperparametersforSeTAR.Temperatureissetto1exceptforSwin-basewithEnergy
score,whereitissetto0.1.
Backbone Dataset Œª top-K
ImageNet1K 0.10 300
CLIP-base
Pascal-VOC 0.05 4
ImageNet1K 0.50 300
CLIP-large
Pascal-VOC 0.30 6
Swin-base ImageNet1K 0.01 700
Table13: HyperparametersforSeTAR+FTandLoRAonImageNet1K.Temperatureissetto1
exceptforSwin-basewithEnergyscore,whichissetto0.1.
Backbone Œª top-K LR Epoch RankforLoRA AlphaforLoRA
CLIP-base 0.10 300 0.01 5 32 16
CLIP-large 0.50 300 0.01 5 64 16
Swin-base 0.01 700 0.01 5 112 16
11https://github.com/AtsuMiyai/LoCoOp
17
F MoreDetailedExperimentResults
Inthissection,wepresentadditionaldetailedresultsfromthemainpaper. Thisincludesthedetailed
resultsoffine-tunedbaselinesontheImageNet1KbenchmarkinTable14;detailedablationresults
onmodality,W ,Œª,andtop-KinTable15,Table16,Table19,andTable21;anddetailedresults
p
ofSeTARwithdifferentsearchalgorithms,prunestrategiesandbackbonesinTable18,Table20,
Table17andTable22.
Table14: DetailresultsofFPR95(FPR)andAUROC(AUC)comparedwithfine-tunedbaselines
onImageNet1Kbenchmark. ‚Ä†iscitedfromTaoetal.(2023). ‚àódenotestheresultsofourre-run.
iNaturalist SUN Places Texture Average
Method
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
CLIP-base
MCMScore
NPOS‚Ä† 19.59 95.68 48.26 89.70 49.82 88.77 51.12 87.58 42.20 90.43
CoOp‚Ä† 43.38 91.26 38.53 91.95 46.68 89.09 50.64 87.83 44.81 90.03
LoCoOp‚Ä† 38.49 92.49 33.27 93.67 39.23 91.07 49.25 89.13 40.17 91.53
LoCoOp‚àó 31.33 93.64 33.68 93.37 42.31 90.10 51.72 87.75 39.76 91.22
LoRA‚àó 30.50 94.51 35.08 92.87 43.20 90.03 57.91 85.97 41.67 90.85
SeTAR+FT 32.95 93.41 30.26 93.81 38.56 91.24 53.32 87.72 38.77 91.55
GL-MCMScore
NPOS‚Ä† 18.70 95.36 38.99 90.33 41.86 89.36 47.89 86.44 36.86 90.37
CoOp‚Ä† 21.30 95.27 31.66 92.16 40.44 89.31 52.93 84.25 36.58 90.25
LoCoOp‚Ä† 24.61 94.89 25.62 94.59 34.00 92.12 49.86 87.49 33.52 92.14
LoCoOp‚àó 18.97 95.90 27.33 94.31 37.29 90.75 52.98 85.95 34.14 91.73
LoRA‚àó 15.16 96.48 27.99 93.48 36.74 90.30 57.56 83.24 34.36 90.88
SeTAR+FT 21.62 95.43 23.38 94.89 32.60 91.93 51.18 87.01 32.19 92.31
CLIP-large
MCMScore
LoCoOp‚àó 41.84 91.77 35.28 92.78 41.52 90.01 44.33 89.96 40.74 91.13
LoRA‚àó 34.65 93.65 29.78 94.21 36.65 91.59 53.40 87.18 38.62 91.66
SeTAR+FT 22.41 95.83 40.07 91.98 45.19 90.13 31.37 93.48 34.75 92.86
GL-MCMScore
LoCoOp‚àó 51.56 89.45 37.85 92.43 43.86 89.33 53.72 86.05 46.74 89.32
LoRA‚àó 41.00 91.96 31.69 93.85 39.65 90.79 61.22 82.46 43.39 89.76
SeTAR+FT 36.56 91.93 34.81 93.08 41.08 90.66 35.74 91.66 37.05 91.83
Swin-base
MSPScore
LoRA‚àó 43.14 87.02 62.66 78.04 67.95 74.90 54.34 81.99 57.02 80.49
SeTAR+FT 29.10 94.38 52.39 86.75 57.67 85.80 49.31 84.28 47.12 87.80
EnergyScore
LoRA‚àó 62.49 71.48 65.05 71.47 75.00 63.24 46.13 85.02 62.17 72.80
SeTAR+FT 29.76 91.56 42.76 87.06 51.73 82.85 32.90 90.56 39.29 88.01
18
Table15: Detailresultsofablationstudyonmodality. WeuseCLIP-B/16asabackbone.
iNaturalist SUN Places Texture ImageNet22K COCO Average
Method
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
ImageNet1K
MCMScore
Visual 29.69 94.58 35.15 92.99 41.25 90.45 55.00 86.92 - - - - 40.27 91.24
Text 30.21 94.33 38.39 92.27 44.48 89.74 58.05 85.64 - - - - 42.78 90.50
Visual+Text 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
GL-MCMScore
Visual 13.81 96.93 27.89 93.67 36.12 90.74 54.06 85.06 - - - - 32.97 91.60
Text 15.44 96.54 30.77 92.78 38.95 89.71 58.14 83.17 - - - - 35.82 90.55
Visual+Text 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
Pascal-VOC
MCMScore
Visual 4.13 98.63 26.31 94.58 30.44 92.58 42.48 93.20 45.19 92.36 50.60 89.36 33.19 93.45
Text 7.29 98.06 26.33 94.68 30.25 92.65 44.57 92.25 44.38 92.40 48.00 90.45 33.47 93.42
Visual+Text 4.59 98.71 24.91 95.15 28.46 93.21 40.44 93.58 48.25 92.08 48.10 89.70 32.46 93.74
GL-MCMScore
Visual 3.90 98.89 22.40 94.27 26.22 93.03 22.87 95.97 31.40 94.10 42.50 90.81 24.88 94.51
Text 3.55 99.01 21.26 94.48 24.87 92.96 30.89 94.07 29.86 94.49 37.10 92.09 24.59 94.52
Visual+Text 3.66 98.96 21.93 94.81 25.04 93.62 20.35 96.36 31.47 94.31 40.70 91.19 23.86 94.87
Table16: DetailresultsofSeTARwithandwithoutconsideringprojectionmatrixW . Weuse
p
CLIP-B/16asabackbone.
iNaturalist SUN Places Texture ImageNet22K COCO Average
Method
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
ImageNet1K
MCMScore
VanillaMCM 32.07 94.43 38.65 92.37 43.73 90.03 57.89 86.13 - - - - 43.09 90.74
SeTARwWp 35.21 93.06 33.50 93.16 41.02 90.50 57.41 86.22 - - - - 41.79 90.74
SeTARw/oWp 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
GL-MCMScore
VanillaGL-MCM 15.34 96.62 30.65 93.01 37.76 90.07 57.41 83.73 - - - - 35.29 90.86
SeTARwWp 19.08 95.69 26.52 93.93 35.18 91.01 56.42 84.34 - - - - 34.30 91.24
SeTARw/oWp 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
Pascal-VOC
MCMScore
VanillaMCM 7.24 98.23 27.91 94.56 32.40 92.45 51.61 91.89 50.60 91.42 53.70 89.30 37.24 92.98
SeTARwWp 6.54 98.40 26.95 94.88 30.61 92.91 49.40 92.09 51.16 91.84 51.00 89.83 35.94 93.32
SeTARw/oWp 4.59 98.71 24.91 95.15 28.46 93.21 40.44 93.58 48.25 92.08 48.10 89.70 32.46 93.74
GL-MCMScore
VanillaGL-MCM 4.33 98.81 22.94 94.63 26.20 93.11 41.61 92.88 37.88 93.17 43.70 90.71 29.44 93.88
SeTARwWp 3.20 98.93 20.73 94.77 23.91 93.53 22.06 95.89 30.65 94.38 39.50 91.41 23.34 94.82
SeTARw/oWp 3.66 98.96 21.93 94.81 25.04 93.62 20.35 96.36 31.47 94.31 40.70 91.19 23.86 94.87
19
Table17: DetailresultsforSeTARwithdifferentbackbones. ‚Ä†iscitedfromJiangetal.(2024). ‚àó
denotestheresultofourre-run.
iNaturalist SUN Places Texture ImageNet22K COCO Average
Method
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
ImageNet1K
CLIP-base
VanillaMCM‚àó 32.07 94.43 38.65 92.37 43.73 90.03 57.89 86.13 - - - - 43.09 90.74
SeTAR+MCM 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
VanillaGL-MCM‚àó 15.34 96.62 30.65 93.01 37.76 90.07 57.41 83.73 - - - - 35.29 90.86
SeTAR+GL-MCM 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
VanillaNegLabel‚Ä† 1.91 99.49 20.53 95.49 35.59 91.64 43.56 90.22 - - - - 25.40 94.21
SeTAR+NegLabel 0.15 99.54 19.06 95.84 30.63 92.22 42.54 90.30 - - - - 23.09 94.48
CLIP-large
VanillaMCM‚àó 28.17 94.97 29.18 94.12 33.66 92.37 57.73 85.46 - - - - 37.19 91.73
SeTAR+MCM 26.96 95.14 27.12 94.54 32.04 92.55 58.90 85.45 - - - - 36.26 91.92
VanillaGL-MCM‚àó 29.58 94.43 32.54 93.35 37.18 91.43 63.28 80.71 - - - - 40.65 89.98
SeTAR+GL-MCM 30.96 94.04 28.72 94.08 34.58 91.89 63.90 80.89 - - - - 39.54 90.22
SwinTransformerV2-base
VanillaMSP‚àó 44.78 89.89 63.12 82.81 67.07 81.45 62.04 82.33 - - - - 59.25 84.12
SeTAR+MSP 41.44 91.08 60.05 85.04 64.31 83.70 58.39 83.26 - - - - 56.05 85.77
VanillaEnergy‚àó 57.52 81.60 71.98 72.93 76.90 68.90 53.65 80.96 - - - - 65.01 76.10
SeTAR+Energy 41.71 89.42 56.53 83.29 62.84 80.20 45.37 84.76 - - - - 51.61 84.42
Pascal-VOC
CLIP-base
VanillaMCM‚àó 7.24 98.23 27.91 94.56 32.40 92.45 51.61 91.89 50.60 91.42 53.70 89.30 37.24 92.98
SeTAR+MCM 4.59 98.71 24.91 95.15 28.46 93.21 40.44 93.58 48.25 92.08 48.10 89.70 32.46 93.74
VanillaGL-MCM‚àó 4.33 98.81 22.94 94.63 26.20 93.11 41.61 92.88 37.88 93.17 43.70 90.71 29.44 93.88
SeTAR+GL-MCM 3.66 98.96 21.93 94.81 25.04 93.62 20.35 96.36 31.47 94.31 40.70 91.19 23.86 94.87
CLIP-large
VanillaMCM‚àó 42.90 94.69 44.27 93.28 41.48 91.57 61.33 89.95 63.37 91.20 59.90 89.40 52.21 91.68
SeTAR+MCM 26.05 96.23 35.97 94.20 33.10 92.45 50.32 91.91 57.69 92.02 52.30 90.67 42.57 92.91
VanillaGL-MCM‚àó 23.29 96.17 40.76 93.49 41.23 91.69 54.98 89.60 53.19 92.67 50.30 91.09 43.96 92.45
SeTAR+GL-MCM 9.62 97.51 27.75 94.73 28.85 92.99 41.77 92.40 39.42 93.98 39.30 92.38 31.12 94.00
Table18: Detailresultsfordifferentsearchalgorithms. HereLESstandsforlayer-exhaustive
greedysearch,MISstandsformodality-interleavegreedysearch,andSeTAR-Sstandsforthesearch
algorithmofSeTAR,whichsearchesvisionandtextlayerssequentially. WeuseCLIP-B/16asa
backbone.
iNaturalist SUN Places Texture ImageNet22K COCO Average
Method
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
ImageNet1K
MCMScore
LES 30.25 94.26 36.42 92.79 42.97 90.15 58.33 85.89 - - - - 41.99 90.78
MIS 28.63 94.46 35.41 92.80 42.37 90.17 55.78 86.59 - - - - 40.55 91.00
SeTAR-S 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
GL-MCMScore
LES 14.43 96.61 27.81 93.49 36.16 90.51 57.20 83.72 - - - - 33.90 91.08
MIS 14.14 96.76 28.28 93.39 36.86 90.39 54.15 84.64 - - - - 33.36 91.29
SeTAR-S 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
Pascal-VOC
MCMScore
LES 5.20 98.73 26.88 95.03 30.78 92.93 44.73 93.35 50.98 91.97 52.10 89.61 35.11 93.60
MIS 5.82 98.49 25.52 95.04 30.10 92.98 43.95 93.06 50.00 92.06 48.20 89.84 33.93 93.58
SeTAR-S 4.59 98.71 24.91 95.15 28.46 93.21 40.44 93.58 48.25 92.08 48.10 89.70 32.46 93.74
GL-MCMScore
LES 3.89 98.87 21.56 94.56 24.70 93.32 23.35 95.80 32.99 93.82 40.40 91.03 24.48 94.57
MIS 3.53 98.95 20.87 94.77 24.30 93.47 19.91 96.24 29.59 94.40 39.00 91.21 22.87 94.84
SeTAR-S 3.66 98.96 21.93 94.81 25.04 93.62 20.35 96.36 31.47 94.31 40.70 91.19 23.86 94.87
20
Table19: DetailresultsofablationstudyonŒª. WeuseCLIP-B/16asabackbone.
iNaturalist SUN Places Texture ImageNet22K COCO Average
Œª
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
ImageNet1K
MCMScore
0.01 28.31 94.60 36.83 92.74 43.01 90.10 55.85 86.58 - - - - 41.00 91.00
0.05 27.41 94.75 35.91 92.70 42.75 90.15 55.57 86.49 - - - - 40.41 91.02
0.10 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
0.15 34.29 93.66 35.88 92.85 42.34 90.24 58.09 86.01 - - - - 42.65 90.69
0.20 34.89 93.62 35.59 92.88 41.95 90.28 58.19 86.11 - - - - 42.66 90.72
0.25 35.88 93.42 35.48 92.76 42.24 90.18 58.39 85.84 - - - - 43.00 90.55
0.30 37.72 93.26 36.27 92.64 42.35 90.10 58.46 86.03 - - - - 43.70 90.50
GL-MCMScore
0.01 13.98 96.76 29.20 93.17 37.56 90.09 54.10 84.47 - - - - 33.71 91.12
0.05 13.90 96.79 28.84 93.24 37.25 90.32 54.20 84.33 - - - - 33.55 91.17
0.10 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
0.15 16.85 96.12 26.99 93.72 35.14 90.74 56.79 83.90 - - - - 33.94 91.12
0.20 17.21 96.10 27.12 93.70 35.31 90.72 57.22 83.89 - - - - 34.21 91.10
0.25 18.30 95.87 27.55 93.64 36.06 90.58 58.28 83.70 - - - - 35.05 90.95
0.30 17.95 95.98 27.91 93.63 36.14 90.53 57.59 84.03 - - - - 34.90 91.04
Pascal-VOC
MCMScore
0.01 5.58 98.43 25.14 94.94 29.13 93.01 40.41 93.35 47.85 92.12 49.60 89.37 32.95 93.54
0.05 4.59 98.71 24.91 95.15 28.46 93.21 40.44 93.58 48.25 92.08 48.10 89.70 32.46 93.74
0.10 5.44 98.50 24.97 95.06 29.60 93.01 42.55 93.26 48.69 92.28 47.80 89.82 33.18 93.65
0.15 5.97 98.53 26.50 95.07 30.88 93.05 46.22 92.94 50.99 92.07 49.80 89.80 35.06 93.58
0.20 6.11 98.53 26.18 95.08 30.53 93.06 45.43 93.06 50.68 92.16 49.40 89.82 34.72 93.62
0.25 6.41 98.43 26.19 94.99 31.24 92.89 47.36 92.72 50.41 92.13 50.20 89.74 35.30 93.48
0.30 6.81 98.34 26.98 94.80 32.13 92.65 48.67 92.52 50.53 92.14 51.10 89.77 36.04 93.37
GL-MCMScore
0.01 4.42 98.83 22.72 94.73 25.93 93.51 22.07 96.22 32.62 94.27 43.50 90.91 25.21 94.74
0.05 3.66 98.96 21.93 94.81 25.04 93.62 20.35 96.36 31.47 94.31 40.70 91.19 23.86 94.87
0.10 3.79 98.94 21.40 94.76 25.05 93.49 20.74 96.29 30.42 94.48 40.00 91.20 23.57 94.86
0.15 3.50 98.98 20.83 94.84 24.34 93.55 20.57 96.20 29.84 94.42 38.50 91.25 22.93 94.87
0.20 3.50 98.94 20.72 94.74 24.13 93.48 19.95 96.28 29.22 94.46 38.60 91.19 22.69 94.85
0.25 4.14 98.96 21.54 94.85 25.37 93.54 23.37 96.14 32.18 94.51 40.30 91.44 24.48 94.90
0.30 4.15 98.90 21.40 94.63 25.16 93.33 23.01 96.03 31.02 94.44 38.90 91.40 23.94 94.79
Table20: Detailresultsondifferentpruningstrategies. WeuseCLIP-B/16asabackbone.
iNaturalist SUN Places Texture ImageNet22K COCO Average
Method
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
ImageNet1K
MCMScore
Principle 32.07 94.43 38.65 92.37 43.73 90.03 57.89 86.13 - - - - 43.09 90.74
Random 32.07 94.43 38.65 92.37 43.73 90.03 57.89 86.13 - - - - 43.09 90.74
Minor 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
GL-MCMScore
Principle 15.34 96.62 30.65 93.01 37.76 90.07 57.41 83.73 - - - - 35.29 90.86
Random 32.07 94.43 38.65 92.37 43.73 90.03 57.89 86.13 - - - - 43.09 90.74
Minor 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
Pascal-VOC
MCMScore
Principle 9.91 98.01 29.24 93.91 32.89 92.30 54.43 90.30 53.53 91.07 49.20 89.07 38.20 92.44
Random 7.24 98.20 27.45 94.60 32.52 92.43 43.30 93.25 49.89 91.02 52.97 89.06 35.57 93.09
Minor 4.59 98.71 24.91 95.15 28.46 93.21 40.44 93.58 48.25 92.08 48.10 89.70 32.46 93.74
GL-MCMScore
Principle 3.10 98.62 20.07 94.41 22.33 93.38 38.53 92.19 31.61 93.07 36.50 90.34 25.36 93.67
Random 3.47 98.99 20.04 95.46 24.07 93.95 31.76 94.86 35.71 93.67 42.17 91.04 26.20 94.66
Minor 3.66 98.96 21.93 94.81 25.04 93.62 20.35 96.36 31.47 94.31 40.70 91.19 23.86 94.87
21
Table21: Detailresultsofablationstudyontop-K.WeuseCLIP-B/16asabackbone.
iNaturalist SUN Places Texture ImageNet22K COCO Average
K
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
ImageNet1K
MCMScore
0 26.50 94.70 36.22 92.66 43.04 90.10 55.82 86.46 - - - - 40.39 90.98
100 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
200 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
300 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
400 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
500 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
600 26.50 94.70 36.22 92.66 43.04 90.10 55.82 86.46 - - - - 40.39 90.98
700 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
800 26.92 94.67 35.57 92.79 42.64 90.16 55.83 86.58 - - - - 40.24 91.05
900 29.38 94.38 36.02 92.75 42.47 90.24 55.20 86.77 - - - - 40.77 91.03
1000 30.63 94.17 36.24 92.93 42.58 90.24 56.84 86.34 - - - - 41.57 90.92
GL-MCMScore
0 14.02 96.80 28.32 93.40 36.91 90.52 54.68 84.32 - - - - 33.48 91.26
100 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
200 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
300 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
400 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
500 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
600 14.02 96.80 28.32 93.40 36.91 90.52 54.68 84.32 - - - - 33.48 91.26
700 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
800 13.36 96.92 28.17 93.36 36.80 90.40 54.17 84.59 - - - - 33.12 91.32
900 14.71 96.63 28.64 93.31 36.56 90.41 54.04 84.78 - - - - 33.49 91.28
1000 15.82 96.42 28.61 93.46 37.20 90.40 54.75 84.35 - - - - 34.10 91.16
Pascal-VOC
MCMScore
0 5.58 98.43 25.14 94.94 29.13 93.01 40.41 93.35 47.85 92.12 49.60 89.37 32.95 93.54
2 4.59 98.71 24.91 95.15 28.46 93.21 40.44 93.58 48.25 92.08 48.10 89.70 32.46 93.74
4 4.59 98.71 24.91 95.15 28.46 93.21 40.44 93.58 48.25 92.08 48.10 89.70 32.46 93.74
6 5.58 98.43 25.14 94.94 29.13 93.01 40.41 93.35 47.85 92.12 49.60 89.37 32.95 93.54
8 5.27 98.45 24.26 94.98 28.31 93.06 39.61 93.31 46.99 92.11 48.10 89.38 32.09 93.55
10 5.58 98.43 25.14 94.94 29.13 93.01 40.41 93.35 47.85 92.12 49.60 89.37 32.95 93.54
12 4.59 98.71 24.91 95.15 28.46 93.21 40.44 93.58 48.25 92.08 48.10 89.70 32.46 93.74
14 5.58 98.43 25.14 94.94 29.13 93.01 40.41 93.35 47.85 92.12 49.60 89.37 32.95 93.54
GL-MCMScore
0 4.42 98.83 22.72 94.73 25.93 93.51 22.07 96.22 32.62 94.27 43.50 90.91 25.21 94.74
2 3.66 98.96 21.93 94.81 25.04 93.62 20.35 96.36 31.47 94.31 40.70 91.19 23.86 94.87
4 3.66 98.96 21.93 94.81 25.04 93.62 20.35 96.36 31.47 94.31 40.70 91.19 23.86 94.87
6 4.42 98.83 22.72 94.73 25.93 93.51 22.07 96.22 32.62 94.27 43.50 90.91 25.21 94.74
8 4.47 98.84 22.76 94.79 25.99 93.56 22.39 96.19 32.85 94.27 43.30 90.95 25.29 94.76
10 4.42 98.83 22.72 94.73 25.93 93.51 22.07 96.22 32.62 94.27 43.50 90.91 25.21 94.74
12 3.66 98.96 21.93 94.81 25.04 93.62 20.35 96.36 31.47 94.31 40.70 91.19 23.86 94.87
14 4.42 98.83 22.72 94.73 25.93 93.51 22.07 96.22 32.62 94.27 43.50 90.91 25.21 94.74
Table22: DetailresultsofResNet50. WeuseImageNet1KastheIDdataset. ‚Ä†iscitedfromDjurisic
etal.(2023).
iNaturalist SUN Places Texture Average
Method
FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë FPR‚Üì AUC‚Üë
Softmax‚Ä† 54.99 87.74 70.83 80.86 73.99 79.76 68.00 79.61 66.95 81.99
Energy‚Ä† 55.72 89.95 59.26 85.89 64.92 82.86 53.72 85.99 58.41 86.17
ReAct‚Ä† 20.38 96.22 24.20 94.20 33.85 91.58 47.30 89.80 31.43 92.95
DICE‚Ä† 25.63 94.49 35.15 90.83 46.49 87.48 31.72 90.30 34.75 90.77
ASH-P‚Ä† 44.57 92.51 52.88 88.35 61.79 61.79 42.06 89.70 50.32 89.04
ASH-B‚Ä† 14.21 97.32 22.08 95.10 33.45 92.31 21.17 95.50 22.73 95.06
ASH-S‚Ä† 11.49 97.87 27.98 94.02 39.78 90.98 11.93 97.60 22.80 95.12
SeTAR 10.08 98.11 27.68 94.15 39.22 91.24 12.54 97.51 22.38 95.25
22
91.2
91.0
90.8
90.6
90.4
90.2
90.0
0 5 10 15 20 25
Num. of visited layers
CORUA
.gvA
46
45
44
43
42
41
40
0 5 10 15 20 25
Num. of visited layers
59RPF
.gvA
Wup
Wdown
Wq
Wk
Wv
Wout
baseline
(a) MCMscore
91.5
91.0
90.5
90.0
0 5 10 15 20 25
Num. of visited layers
CORUA
.gvA
39
38
37
36
35
34
33
0 5 10 15 20 25
Num. of visited layers
59RPF
.gvA
Wup
Wdown
Wq
Wk
Wv
Wout
baseline
(b) GL-MCMscore
Figure2: AverageAUROC/FPR95ofdifferentweighttypesonImageNet1Kbenchmark. We
useCLIP-B/16asabackbone.
93.5
93.0
92.5
92.0
0 5 10 15 20 25
Num. of visited layers
CORUA
.gvA
42
40
38
36
34
32
0 5 10 15 20 25
Num. of visited layers
59RPF
.gvA
Wup
Wdown
Wq
Wk
Wv
Wout
baseline
(a) MCMscore
95.00
94.75
94.50
94.25
94.00
93.75
93.50
93.25
0 5 10 15 20 25
Num. of visited layers
CORUA
.gvA
32
30
28
26
24
0 5 10 15 20 25
Num. of visited layers
59RPF
.gvA
Wup
Wdown
Wq
Wk
Wv
Wout
baseline
(b) GL-MCMscore
Figure3: AverageAUROC/FPR95ofdifferentweighttypesonPascal-VOCbenchmark. Weuse
CLIP-B/16asabackbone.
23
91.2
91.0
90.8
90.6
0.00 0.05 0.10 0.15 0.20 0.25 0.30
The Value of
CORUA
.gvA
44
42
40
38
36
34
0.00 0.05 0.10 0.15 0.20 0.25 0.30
The Value of
59RPF
.gvA
SeTAR+MCM
SeTAR+GL-MCM
(a) ImageNet1K
94.75
94.50
94.25
94.00
93.75
93.50
0.00 0.05 0.10 0.15 0.20 0.25 0.30
The Value of
CORUA
.gvA
36
34
32
30
28
26
24
0.00 0.05 0.10 0.15 0.20 0.25 0.30
The Value of
59RPF
.gvA
SeTAR+MCM
SeTAR+GL-MCM
(b) Pascal-VOC
Figure4: AblationstudiesonŒªondifferentIDdatasets. WeuseCLIP-B/16asabackbone.
91.3
91.2
91.1
91.0
90.9
0 200 400 600 800 1000
The Value of K
CORUA
.gvA
40
38
36
34
0 200 400 600 800 1000
The Value of K
59RPF
.gvA
SeTAR+MCM
SeTAR+GL-MCM
(a) ImageNet1K
94.8
94.6
94.4
94.2 94.0
93.8
93.6
0 2 4 6 8 10 12 14
The Value of K
CORUA
.gvA
32
30
28
26
24
0 2 4 6 8 10 12 14
The Value of K
59RPF
.gvA
SeTAR+MCM
SeTAR+GL-MCM
(b) Pascal-VOC
Figure5: Ablationstudiesontop-KondifferentIDdatasets. WeuseCLIP-B/16asabackbone.
24
loss_total loss_id loss_ood
1.0 S Loe RTA AR+FT 1.4 S Loe RTA AR+FT 4.4 S Loe RTA AR+FT
0.8
1.2 4.6
0.6
1.0 4.8
0.4
0.8 5.0
0.2
0.6 5.2
0.0
0.4 5.4
0.2
0.2 5.6
0.4
0 5 10 15 0 5 10 15 0 5 10 15
Epoch Epoch Epoch
(a) LoCoOpLoss (b) IDLoss (c) OODLoss
Figure6: LossplotsofSeTAR+FTv.s. LoRAonImageNet1K.WeuseCLIP-B/16asabackbone.
SeTAR+FT demonstrates faster convergence across all losses, especially in the OOD loss. For
reference,withMCMscore,SeTAR+FTachievesanaverageFPRof38.77atepoch5. WhileLoRA
achievesanaverageFPRof42.88,39.92and42.23atepoch1,5and15,respectively.
V-11 V-10 V-9 V-8 V-7 V-6 V-5 V-4 V-3 V-2 V-1 V-0 T-11 T-10 T-9 T-8 T-7 T-6 T-5 T-4 T-3 T-2 T-1 T-0
Modality-LayerIndex
K1NI
COV
0.15 0.15 0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.00 0.00 0.15 0.00 0.00 0.00 0.00 0.00 0.25 0.10 0.20 0.05 0.05 0.00 0.20
0.10 0.35 0.40 0.05 0.25 0.35 0.10 0.00 0.00 0.00 0.25 0.35 0.00 0.25 0.05 0.10 0.20 0.05 0.05 0.00 0.10 0.00 0.25 0.00
(a) CLIP-base
V-23 V-22 V-21 V-20 V-19 V-18 V-17 V-16 V-15 V-14 V-13 V-12 V-11 V-10 V-9 V-8 V-7 V-6 V-5 V-4 V-3 V-2 V-1 V-0 T-11 T-10 T-9 T-8 T-7 T-6 T-5 T-4 T-3 T-2 T-1 T-0
Modality-LayerIndex
K1NI
COV
0.00 0.00 0.00 0.05 0.15 0.10 0.00 0.00 0.05 0.10 0.20 0.00 0.15 0.00 0.10 0.00 0.00 0.05 0.00 0.00 0.00 0.15 0.00 0.40 0.35 0.00 0.00 0.00 0.10 0.05 0.35 0.00 0.20 0.00 0.05 0.20
0.40 0.35 0.00 0.20 0.00 0.30 0.25 0.10 0.05 0.25 0.05 0.30 0.05 0.10 0.15 0.05 0.20 0.15 0.30 0.10 0.10 0.20 0.10 0.35 0.00 0.05 0.05 0.40 0.00 0.15 0.00 0.30 0.40 0.05 0.05 0.40
(b) CLIP-large
3-1 3-0 2-17 2-16 2-15 2-14 2-13 2-12 2-11 2-10 2-9 2-8 2-7 2-6 2-5 2-4 2-3 2-2 2-1 2-0 1-1 1-0 0-1 0-0
StageIndex-BlockIndex
K1NI
0.40 0.40 0.40 0.40 0.40 0.00 0.00 0.00 0.00 0.10 0.00 0.00 0.00 0.25 0.00 0.00 0.00 0.00 0.00 0.00 0.40 0.30 0.00 0.10
(c) Swin-base
Figure7: VisualizationofSeTARrankreductionratiodistributionondifferentIDdatasetswith
differentbackbones. IN1K,VOCstandforImageNet1KandPascal-VOC.AndV,Tstandforvisual
modalityandtextmodalityoftheCLIPmodel.
25
tower_type weight_type layer_num best_ratio total_loss* id_loss ood_loss val_acc ood_patch_percent
step
0 visual W_up 11 0.15 0.647777 1.093326 -4.455494 71.399998 38.906631
1 visual W_up 10 0.15 0.644654 1.083629 -4.389751 71.799998 39.293876
2 visual W_up 9 0.00 0.644654 1.083629 -4.389751 71.799998 39.293876
3 visual W_up 8 0.00 0.644654 1.083629 -4.389751 71.799998 39.293876
4 visual W_up 7 0.00 0.644654 1.083629 -4.389751 71.799998 39.293876
5 visual W_up 6 0.00 0.644654 1.083629 -4.389751 71.799998 39.293876
6 visual W_up 5 0.00 0.644654 1.083629 -4.389751 71.799998 39.293876
7 visual W_up 4 0.00 0.644654 1.083629 -4.389751 71.799998 39.293876
8 visual W_up 3 0.05 0.640844 1.079729 -4.388844 71.999998 39.209695
9 visual W_up 2 0.00 0.640844 1.079729 -4.388844 71.999998 39.209695
10 visual W_up 1 0.00 0.640844 1.079729 -4.388844 71.999998 39.209695
11 visual W_up 0 0.15 0.640132 1.079109 -4.389775 72.199998 39.156123
12 text W_up 11 0.00 0.640132 1.079109 -4.389775 72.199998 39.156123
13 text W_up 10 0.00 0.640132 1.079109 -4.389775 72.199998 39.156123
14 text W_up 9 0.00 0.640132 1.079109 -4.389775 72.199998 39.156123
15 text W_up 8 0.00 0.640132 1.079109 -4.389775 72.199998 39.156123
16 text W_up 7 0.00 0.640132 1.079109 -4.389775 72.199998 39.156123
17 text W_up 6 0.25 0.630751 1.075123 -4.443716 71.600001 38.808673
18 text W_up 5 0.10 0.630514 1.078703 -4.481889 71.599997 38.246428
19 text W_up 4 0.20 0.622065 1.075958 -4.538932 72.000001 38.452552
20 text W_up 3 0.05 0.620440 1.079326 -4.588857 71.999997 38.649488
21 text W_up 2 0.05 0.618521 1.076858 -4.583368 71.600001 38.444899
22 text W_up 1 0.00 0.618521 1.076858 -4.583368 71.600001 38.444899
23 text W_up 0 0.20 0.615174 1.069851 -4.546776 72.499997 38.642345
Listing1: ExampleprocedureofSeTARonImageNet1KwithCLIP-base. Wesearchthevisual
andtexttowerfromtoptobottom. Ateachstep,weselectthebestratiothatminimizestheloss.
26
NeurIPSPaperChecklist
1. Claims
Question: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe
paper‚Äôscontributionsandscope?
Answer: [Yes]
Justification: Wehavecarefullycraftedtheabstractandintroductiontoaccuratelyreflectthe
contributionsandscopeofthepaper. Specifically,weproposeanoveltraining-freemethod,
SeTARwithafinetuningextensionSeTAR+FT,anddemonstrateitseffectivenessforOOD
detectiontasks.
Guidelines:
‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
madeinthepaper.
‚Ä¢ Theabstractand/orintroductionshouldclearlystatetheclaimsmade,includingthe
contributionsmadeinthepaperandimportantassumptionsandlimitations. ANoor
NAanswertothisquestionwillnotbeperceivedwellbythereviewers.
‚Ä¢ Theclaimsmadeshouldmatchtheoreticalandexperimentalresults,andreflecthow
muchtheresultscanbeexpectedtogeneralizetoothersettings.
‚Ä¢ Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals
arenotattainedbythepaper.
2. Limitations
Question: Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?
Answer: [Yes]
Justification: LimitationsoftheproposedmethodarediscussedinAppendixA.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperhasnolimitationwhiletheanswerNomeansthat
thepaperhaslimitations,butthosearenotdiscussedinthepaper.
‚Ä¢ Theauthorsareencouragedtocreateaseparate"Limitations"sectionintheirpaper.
‚Ä¢ Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto
violationsoftheseassumptions(e.g.,independenceassumptions,noiselesssettings,
modelwell-specification,asymptoticapproximationsonlyholdinglocally).Theauthors
shouldreflectonhowtheseassumptionsmightbeviolatedinpracticeandwhatthe
implicationswouldbe.
‚Ä¢ Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwas
onlytestedonafewdatasetsorwithafewruns. Ingeneral,empiricalresultsoften
dependonimplicitassumptions,whichshouldbearticulated.
‚Ä¢ Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.
Forexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution
isloworimagesaretakeninlowlighting. Oraspeech-to-textsystemmightnotbe
usedreliablytoprovideclosedcaptionsforonlinelecturesbecauseitfailstohandle
technicaljargon.
‚Ä¢ Theauthorsshoulddiscussthecomputationalefficiencyoftheproposedalgorithms
andhowtheyscalewithdatasetsize.
‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
addressproblemsofprivacyandfairness.
‚Ä¢ Whiletheauthorsmightfearthatcompletehonestyaboutlimitationsmightbeusedby
reviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover
limitationsthataren‚Äôtacknowledgedinthepaper. Theauthorsshouldusetheirbest
judgmentandrecognizethatindividualactionsinfavoroftransparencyplayanimpor-
tantroleindevelopingnormsthatpreservetheintegrityofthecommunity. Reviewers
willbespecificallyinstructedtonotpenalizehonestyconcerninglimitations.
3. TheoryAssumptionsandProofs
Question: Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsand
acomplete(andcorrect)proof?
27
Answer: [NA]
Justification: Thepaperdoesnotincludetheoreticalresults.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludetheoreticalresults.
‚Ä¢ Allthetheorems, formulas, andproofsinthepapershouldbenumberedandcross-
referenced.
‚Ä¢ Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.
‚Ä¢ Theproofscaneitherappearinthemainpaperorthesupplementalmaterial, butif
theyappearinthesupplementalmaterial,theauthorsareencouragedtoprovideashort
proofsketchtoprovideintuition.
‚Ä¢ Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented
byformalproofsprovidedinappendixorsupplementalmaterial.
‚Ä¢ TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.
4. ExperimentalResultReproducibility
Question: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-
perimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions
ofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?
Answer: [Yes]
Justification: The paper includes detailed experimental and hyperparameters settings in
Section4.1andAppendixE.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
‚Ä¢ Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceived
well by the reviewers: Making the paper reproducible is important, regardless of
whetherthecodeanddataareprovidedornot.
‚Ä¢ Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken
tomaketheirresultsreproducibleorverifiable.
‚Ä¢ Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.
Forexample,ifthecontributionisanovelarchitecture,describingthearchitecturefully
mightsuffice,orifthecontributionisaspecificmodelandempiricalevaluation,itmay
benecessarytoeithermakeitpossibleforotherstoreplicatethemodelwiththesame
dataset,orprovideaccesstothemodel. Ingeneral. releasingcodeanddataisoften
onegoodwaytoaccomplishthis,butreproducibilitycanalsobeprovidedviadetailed
instructionsforhowtoreplicatetheresults,accesstoahostedmodel(e.g.,inthecase
ofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare
appropriatetotheresearchperformed.
‚Ä¢ WhileNeurIPSdoesnotrequirereleasingcode,theconferencedoesrequireallsubmis-
sionstoprovidesomereasonableavenueforreproducibility,whichmaydependonthe
natureofthecontribution. Forexample
(a) Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow
toreproducethatalgorithm.
(b) Ifthecontributionisprimarilyanewmodelarchitecture,thepapershoulddescribe
thearchitectureclearlyandfully.
(c) Ifthecontributionisanewmodel(e.g.,alargelanguagemodel),thenthereshould
eitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce
themodel(e.g.,withanopen-sourcedatasetorinstructionsforhowtoconstruct
thedataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authorsarewelcometodescribetheparticularwaytheyprovideforreproducibility.
Inthecaseofclosed-sourcemodels,itmaybethataccesstothemodelislimitedin
someway(e.g.,toregisteredusers),butitshouldbepossibleforotherresearchers
tohavesomepathtoreproducingorverifyingtheresults.
5. Openaccesstodataandcode
28
Question: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstruc-
tionstofaithfullyreproducethemainexperimentalresults,asdescribedinsupplemental
material?
Answer: [Yes]
Justification: Codeareavailableat https://github.com/X1AOX1A/SeTAR.
Guidelines:
‚Ä¢ TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.
‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy)formoredetails.
‚Ä¢ Whileweencouragethereleaseofcodeanddata,weunderstandthatthismightnotbe
possible,so‚ÄúNo‚Äùisanacceptableanswer. Paperscannotberejectedsimplyfornot
includingcode,unlessthisiscentraltothecontribution(e.g.,foranewopen-source
benchmark).
‚Ä¢ Theinstructionsshouldcontaintheexactcommandandenvironmentneededtorunto
reproducetheresults. SeetheNeurIPScodeanddatasubmissionguidelines(https:
//nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.
‚Ä¢ Theauthorsshouldprovideinstructionsondataaccessandpreparation,includinghow
toaccesstherawdata,preprocesseddata,intermediatedata,andgenerateddata,etc.
‚Ä¢ Theauthorsshouldprovidescriptstoreproduceallexperimentalresultsforthenew
proposedmethodandbaselines. Ifonlyasubsetofexperimentsarereproducible,they
shouldstatewhichonesareomittedfromthescriptandwhy.
‚Ä¢ Atsubmissiontime, topreserveanonymity, theauthorsshouldreleaseanonymized
versions(ifapplicable).
‚Ä¢ Providingasmuchinformationaspossibleinsupplementalmaterial(appendedtothe
paper)isrecommended,butincludingURLstodataandcodeispermitted.
6. ExperimentalSetting/Details
Question: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: WeprovidedetailedexperimentalsettingsinSection4.1andAppendixE.We
givethedetailsofourdesignchoicesinSection4.4anddatasetsinAppendixC.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
‚Ä¢ Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail
thatisnecessarytoappreciatetheresultsandmakesenseofthem.
‚Ä¢ Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental
material.
7. ExperimentStatisticalSignificance
Question:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate
informationaboutthestatisticalsignificanceoftheexperiments?
Answer: [Yes]
Justification: Wereporttheresultswithstandarddeviationfromrunsof3seeds.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
‚Ä¢ Theauthorsshouldanswer"Yes"iftheresultsareaccompaniedbyerrorbars,confi-
denceintervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupport
themainclaimsofthepaper.
‚Ä¢ Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for
example,train/testsplit,initialization,randomdrawingofsomeparameter,oroverall
runwithgivenexperimentalconditions).
29
‚Ä¢ Themethodforcalculatingtheerrorbarsshouldbeexplained(closedformformula,
calltoalibraryfunction,bootstrap,etc.)
‚Ä¢ Theassumptionsmadeshouldbegiven(e.g.,Normallydistributederrors).
‚Ä¢ Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderror
ofthemean.
‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis
ofNormalityoferrorsisnotverified.
‚Ä¢ Forasymmetricdistributions,theauthorsshouldbecarefulnottoshowintablesor
figuressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative
errorrates).
‚Ä¢ Iferrorbarsarereportedintablesorplots,Theauthorsshouldexplaininthetexthow
theywerecalculatedandreferencethecorrespondingfiguresortablesinthetext.
8. ExperimentsComputeResources
Question: Foreachexperiment,doesthepaperprovidesufficientinformationonthecom-
puterresources(typeofcomputeworkers,memory,timeofexecution)neededtoreproduce
theexperiments?
Answer: [Yes]
Justification: WeprovidethedetailsofthecomputeresourcesinSection4.1.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
‚Ä¢ ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,
orcloudprovider,includingrelevantmemoryandstorage.
‚Ä¢ Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual
experimentalrunsaswellasestimatethetotalcompute.
‚Ä¢ Thepapershoulddisclosewhetherthefullresearchprojectrequiredmorecompute
thantheexperimentsreportedinthepaper(e.g.,preliminaryorfailedexperimentsthat
didn‚Äôtmakeitintothepaper).
9. CodeOfEthics
Question: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe
NeurIPSCodeofEthicshttps://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: WeprovidetheethicalconsiderationsinAppendixA.
Guidelines:
‚Ä¢ TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeofEthics.
‚Ä¢ IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthatrequirea
deviationfromtheCodeofEthics.
‚Ä¢ Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsid-
erationduetolawsorregulationsintheirjurisdiction).
10. BroaderImpacts
Question: Does the paper discuss both potential positive societal impacts and negative
societalimpactsoftheworkperformed?
Answer: [Yes]
Justification: WeprovidethebroaderimpactsinAppendixA.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthereisnosocietalimpactoftheworkperformed.
‚Ä¢ IftheauthorsanswerNAorNo,theyshouldexplainwhytheirworkhasnosocietal
impactorwhythepaperdoesnotaddresssocietalimpact.
‚Ä¢ Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses
(e.g.,disinformation,generatingfakeprofiles,surveillance),fairnessconsiderations
(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific
groups),privacyconsiderations,andsecurityconsiderations.
30
‚Ä¢ Theconferenceexpectsthatmanypaperswillbefoundationalresearchandnottied
toparticularapplications,letalonedeployments. However,ifthereisadirectpathto
anynegativeapplications,theauthorsshouldpointitout. Forexample,itislegitimate
topointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto
generatedeepfakesfordisinformation. Ontheotherhand,itisnotneededtopointout
thatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain
modelsthatgenerateDeepfakesfaster.
‚Ä¢ Theauthorsshouldconsiderpossibleharmsthatcouldarisewhenthetechnologyis
being used as intended and functioning correctly, harms that could arise when the
technologyisbeingusedasintendedbutgivesincorrectresults,andharmsfollowing
from(intentionalorunintentional)misuseofthetechnology.
‚Ä¢ Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanismsformonitoringmisuse,mechanismstomonitorhowasystemlearnsfrom
feedbackovertime,improvingtheefficiencyandaccessibilityofML).
11. Safeguards
Question: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible
releaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrainedlanguagemodels,
imagegenerators,orscrapeddatasets)?
Answer: [NA]
Justification: Ourworkdoesnotdirectlyprovidepre-trainedmodelsorscrapeddatasets.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperposesnosuchrisks.
‚Ä¢ Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith
necessarysafeguardstoallowforcontrolleduseofthemodel,forexamplebyrequiring
thatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing
safetyfilters.
‚Ä¢ DatasetsthathavebeenscrapedfromtheInternetcouldposesafetyrisks. Theauthors
shoulddescribehowtheyavoidedreleasingunsafeimages.
‚Ä¢ Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdo
notrequirethis,butweencourageauthorstotakethisintoaccountandmakeabest
faitheffort.
12. Licensesforexistingassets
Question: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin
thepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand
properlyrespected?
Answer: [Yes]
Justification: WeincludethecitationandURLofmodelsanddatasetsusedinthepaper.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotuseexistingassets.
‚Ä¢ Theauthorsshouldcitetheoriginalpaperthatproducedthecodepackageordataset.
‚Ä¢ Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea
URL.
‚Ä¢ Thenameofthelicense(e.g.,CC-BY4.0)shouldbeincludedforeachasset.
‚Ä¢ Forscrapeddatafromaparticularsource(e.g.,website),thecopyrightandtermsof
serviceofthatsourceshouldbeprovided.
‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
packageshouldbeprovided. Forpopulardatasets,paperswithcode.com/datasets
hascuratedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethe
licenseofadataset.
‚Ä¢ Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseof
thederivedasset(ifithaschanged)shouldbeprovided.
31
‚Ä¢ Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachoutto
theasset‚Äôscreators.
13. NewAssets
Question:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation
providedalongsidetheassets?
Answer: [NA]
Justification: Wedonotintroducenewassetsinthepaper.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotreleasenewassets.
‚Ä¢ Researchersshouldcommunicatethedetailsofthedataset/code/modelaspartoftheir
submissions via structured templates. This includes details about training, license,
limitations,etc.
‚Ä¢ Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose
assetisused.
‚Ä¢ Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable). Youcaneither
createananonymizedURLorincludeananonymizedzipfile.
14. CrowdsourcingandResearchwithHumanSubjects
Question: Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper
includethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as
wellasdetailsaboutcompensation(ifany)?
Answer: [NA]
Justification: Ourworkdoesnotinvolvecrowdsourcingnorresearchwithhumansubjects.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
‚Ä¢ Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontribu-
tionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe
includedinthemainpaper.
‚Ä¢ AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,
orotherlaborshouldbepaidatleasttheminimumwageinthecountryofthedata
collector.
15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman
Subjects
Question: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether
suchrisksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)
approvals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor
institution)wereobtained?
Answer: [NA]
Justification: Ourworkdoesnotinvolvecrowdsourcingnorresearchwithhumansubjects.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
‚Ä¢ Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)
mayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you
shouldclearlystatethisinthepaper.
‚Ä¢ Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions
andlocations,andweexpectauthorstoadheretotheNeurIPSCodeofEthicsandthe
guidelinesfortheirinstitution.
‚Ä¢ Forinitialsubmissions,donotincludeanyinformationthatwouldbreakanonymity(if
applicable),suchastheinstitutionconductingthereview.
32
